{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Effect of Forcings on CAMELs Simulations\n",
    "\n",
    "Now we can look at the output and see which forcing variables have the most error. This code is meant to be run on the subset of basins that the ensembles of different model decisions were run on in the `camels_pysumma.ipynb` notebook. This notebook will not run all the way through if the ensembles were not created."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import xarray as xr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### You will need to edit these paths to be your folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some folder places\n",
    "home = '/glade/work/ashleyvb/CAMELs'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "Keep these the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_folder = home+'/summa_camels'\n",
    "settings_folder = top_folder+'/settings.v1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Make problem complexity choices here:\n",
    "\n",
    "In the notebook `camels_pysumma.ipynb` you decided if you wanted to run: \n",
    " - `default_prob = 1`: the default configuration with the parameters you have.\n",
    " - `lhc_prob = 1`: the default configuration with exploration of the parameter space.\n",
    " - `lhc_config_prob = 1`: 8 different configurations (choices that have been seen to affect the model output in previous research) with exploration of the parameter space.\n",
    " \n",
    "DO NOT choose one of these to be one here if you did not choose it to be one before.\n",
    "\n",
    "Again, you can choose all of these to be 1, but each step of complexity will contain the previous problem(s), so it is not necessary. In this notebook it will not make a diffence in the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_prob = 0\n",
    "lhc_prob = 0\n",
    "lhc_config_prob = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# Summary Statistics of Error on Output\n",
    "Let's look at some error metrics by HRU, starting with the default paramaters and default configuration.\n",
    "KGE means perfect agreement if it is 1, and <0 means the mean is a better guess. \n",
    "Bias means perfect aggreement if it is 0, and larger means larger error. \n",
    "All data have a small number added so we don't divide by 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define truth data set\n",
    "if lhc_config_prob==1: \n",
    "    suffix = '_configs_latin.nc'\n",
    "    default_name = '++BallBerry++lightSnow++logBelowCanopy++0'\n",
    "    lat_name = [default_name[:-1]+str(i) for i in range(0,11)]\n",
    "elif lhc_prob==1:\n",
    "    suffix = '_latin.nc'\n",
    "    default_name = '0'  \n",
    "    lat_name = [str(i) for i in range(0,11)]\n",
    "elif default_prob==1:\n",
    "    suffix = '_hru.nc'\n",
    "    default_name = 'default'      \n",
    "sim_truth = xr.open_dataset(top_folder+'/output/merged_day/NLDAStruth'+suffix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get number of HRUs and small add\n",
    "attrib = xr.open_dataset(top_folder+'/settings.v1/attributes.nc')\n",
    "the_hru = np.array(attrib['hruId'])\n",
    "small_add = 1e-10 # so no zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set forcings and create dictionaries\n",
    "forc_sim= ['windspd','SWRadAtm','spechum','pptrate', 'LWRadAtm','airtemp','airpres']\n",
    "comp_sim=['scalarSurfaceRunoff','scalarAquiferBaseflow','scalarInfiltration','scalarRainPlusMelt','scalarSoilDrainage',\n",
    "          'scalarLatHeatTotal','scalarSenHeatTotal','scalarSnowSublimation',\n",
    "          'scalarSWE',\n",
    "          'scalarCanopyWat',\n",
    "          'scalarNetRadiation','scalarTotalET','scalarTotalRunoff','scalarTotalSoilWat']\n",
    "var_sim = np.concatenate([forc_sim, comp_sim])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col1 = ['c','m','orange','b','g','r','gray']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# definitions for KGE computation\n",
    "def covariance(x,y,dims=None):\n",
    "    return xr.dot(x-x.mean(dims), y-y.mean(dims), dims=dims) / x.count(dims)\n",
    "\n",
    "def correlation(x,y,dims=None):\n",
    "    return (covariance(x,y,dims)) / (x.std(dims) * y.std(dims))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if lhc_prob==1 or lhc_config_prob==1: decision_set = np.array(sim_truth['decision']) \n",
    "else: decision_set = np.array('default')\n",
    "#get names off the files\n",
    "decision_set = np.array(sim_truth['decision']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up xarray\n",
    "hrud = sim_truth['hru'] #indices here are 0 to number of basins\n",
    "shape = ( len(decision_set),len(hrud), len(forc_sim))\n",
    "dims = ('decision','hru','var')\n",
    "coords = {'decision':decision_set,'hru': hrud, 'var':forc_sim}\n",
    "error_data = xr.Dataset(coords=coords)\n",
    "for s in comp_sim:\n",
    "    error_data[s] = xr.DataArray(data=np.full(shape, np.nan),\n",
    "                                 coords=coords, dims=dims,\n",
    "                                 name=s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "Now run the actual computations on KGE. \n",
    "This takes 6 minutes on 4 HRUs for the lhc_config problem, and less for the smaller problems. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "truth0_0 = sim_truth.drop_vars('hruId').load()\n",
    "for v in forc_sim:\n",
    "    truth = truth0_0\n",
    "    truth = truth.isel(time = slice(365*24,(365*6+2)*24))+small_add #don't include first year, 5 years\n",
    "    sim = xr.open_dataset(top_folder+'/output/merged_day/NLDASconstant_' + v + suffix)\n",
    "    sim = sim.drop_vars('hruId').load()\n",
    "    sim = sim.isel(time = slice(365*24,(365*6+2)*24))+small_add #don't include first year, 5 years\n",
    "    r = sim.mean(dim='time') #to set up xarray since xr.dot not supported on dataset and have to do loop\n",
    "    for s in var_sim:         \n",
    "        r[s] = correlation(sim[s],truth[s],dims='time')\n",
    "    ds = 1 - np.sqrt( np.square(r-1) \n",
    "        + np.square( sim.std(dim='time')/truth.std(dim='time') - 1) \n",
    "        + np.square( sim.mean(dim='time')/truth.mean(dim='time') - 1) )\n",
    "    ds0 = ds.load()\n",
    "    for s in comp_sim:\n",
    "        error_data[s].loc[:,:,v]  = ds0[s]\n",
    "    print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Now summarize this by totals over all output variables and each output variable, and ranks each constant forcing from most error to least error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup summaries by simulated output variable\n",
    "summ_kind = ['total','rank']\n",
    "shape = (len(decision_set),len(hrud),len(forc_sim),len(summ_kind))\n",
    "dims = ('decision','hru','var','summary')\n",
    "coords = {'decision':decision_set,'hru': the_hru, 'var': forc_sim,'summary':summ_kind}\n",
    "rank_data = xr.Dataset(coords=coords)\n",
    "rank_data['all'] = xr.DataArray(data=np.full(shape, np.nan),\n",
    "                                 coords=coords, dims=dims,\n",
    "                                 name='all')\n",
    "for s in comp_sim:\n",
    "    rank_data[s] = xr.DataArray(data=np.full(shape, np.nan),\n",
    "                                 coords=coords, dims=dims,\n",
    "                                 name=s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate sums of KGE and ranks of worst to best forcing for error over all variables\n",
    "ds = error_data\n",
    "ds =ds.fillna(0) # So don't add to total KGE    \n",
    "ds1 = ds.loc[dict(var = forc_sim)]\n",
    "ds1 = ds1.where(ds1>-1,-1) #make the very negative values be -1\n",
    "ds2 = sum(d for d in ds1.data_vars.values())\n",
    "rank_data['all'].loc[:,:,:,'total'] = ds2.values\n",
    "rank_data['all'].loc[:,:,:,'rank'] = ds2.rank(dim='var').values\n",
    "for s in comp_sim:\n",
    "    rank_data[s].loc[:,:,:,'total'] = ds1[s].values\n",
    "    rank_data[s].loc[:,:,:,'rank'] = ds1[s].rank(dim='var').values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "Let's first look at the error as a total over the simulated variables (14 of them not including the forcings), with just the default parameters and configurations. Each row is a basin here, and the worse the KGE (more error) the shorter the bar. KGE does not need to be normalized. We plot the HRU error as stack of values, with no error plotting as a height of 1 for that color. Values less than 0 are plotted as 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup plots\n",
    "y = np.arange(len(hrud))\n",
    "col_forc = col1\n",
    "letter = list('ABCDEFGHIJKLMNOPQRSTUVWXYZ')\n",
    "wid = np.ceil(len(var_sim)/3)\n",
    "inc = np.floor(len(hrud)/10)\n",
    "if inc<1: inc=1\n",
    "ytic = np.arange(0, len(hrud),inc).tolist()\n",
    "ytic =[int(i) for i in ytic]\n",
    "ytics =[str(i) for i in ytic]\n",
    "labels =[\"V\"+i for i in ytics]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "wid = len(summ_kind)+1\n",
    "plot1 = plt.figure(1, figsize = (15,5))\n",
    "for i, t in enumerate(summ_kind):  \n",
    "    plot2 = plt.subplot(1,wid,i+1)\n",
    "    data = rank_data['all'].loc[default_name,:,:,t]    \n",
    "    for j, v in enumerate(forc_sim):\n",
    "        if t=='total': plt.bar(height = data.loc[:,v]/14, x = j+.5, width = 1.0, color = col_forc[j], bottom = y)\n",
    "        if t=='rank': plt.bar(height = 1, x = data.loc[:,v]-0.5, width = 1.0, color = col_forc[j], bottom = y)\n",
    "\n",
    "    plt.title('('+letter[i]+') '+'Total over Output Variables')\n",
    "    plt.xlim(0,len(forc_sim))       \n",
    "    plt.ylim(0,len(ytic))   \n",
    "    plt.yticks(ytic, labels, fontsize = 9)\n",
    "    plt.xticks(np.arange(0, len(forc_sim)+.01, 1).tolist())\n",
    "    plt.tick_params(axis = \"y\", which = \"both\", bottom = False, top = False)\n",
    "    plt.ylabel(\"CAMELS basin (\"+labels[0]+\"-\"+labels[-1]+\")\", fontsize = 9)\n",
    "    if t=='total': word = \"KGE sum\"\n",
    "    if t=='rank': word = \"Rank (in order of most to least error)\"\n",
    "    plt.xlabel(word, fontsize = 9)\n",
    "\n",
    "plt.subplots_adjust(hspace = .4)\n",
    "\n",
    "for j, v in enumerate(forc_sim):\n",
    "    plt.scatter([],[], color = col_forc[j], label = 'constant_' +  v)\n",
    "plt.figlegend(loc = 'center right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Constant air pressure has almost a perfect KGE in the example basins. Now look at it by simulated variable instead of total. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "wid = np.ceil(len(comp_sim)/3)\n",
    "plot1 = plt.figure(1, figsize = (20,10))\n",
    "\n",
    "for i, s in enumerate(comp_sim):\n",
    "    data = rank_data[s].loc[default_name,:,:,'rank']  \n",
    "    plot2 = plt.subplot(3,wid,i+1)\n",
    "    for j, v in enumerate(forc_sim):\n",
    "        plt.bar(height = 1, x = data.loc[:,v]-0.5, width = 1.0, color = col_forc[j], bottom = y)\n",
    "    \n",
    "    plt.title('('+letter[i]+') '+s[6:])\n",
    "    plt.xlim(0,len(forc_sim))       \n",
    "    plt.ylim(0,len(ytic))   \n",
    "    plt.yticks(ytic, labels, fontsize = 9)\n",
    "    plt.xticks(np.arange(0, len(forc_sim)+.01, 1).tolist())\n",
    "    plt.tick_params(axis = \"y\", which = \"both\", bottom = False, top = False)\n",
    "    plt.ylabel(\"CAMELS basin (\"+labels[0]+\"-\"+labels[-1]+\")\", fontsize = 9)\n",
    "    word = \"Rank (in order of most to least error)\"\n",
    "    plt.xlabel(word, fontsize = 9)\n",
    "\n",
    "plt.subplots_adjust(hspace = .4)\n",
    "\n",
    "for j, v in enumerate(forc_sim):\n",
    "    plt.scatter([],[], color = col_forc[j], label = 'constant_' +  v)\n",
    "plt.figlegend(loc = 'center right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Now we add up the HRUs that have each forcing ranked as 1, 2, ... 7, sequentially for each simulated variable, and plot it as number of HRUS at with a forcing at each ranking. We normalize the sum of the number of HRUs by the number counted, so if there are ties in an HRU's rankings, there will be a fewer HRUs in the count. For example, if their is no Surface Runoff for an HRU with a particular decision, all forcings will result in a perfect KGE of 1, and all will be ranked as 4. Thus, the Rank 1 plot will not have a contribution from this HRU. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup plots\n",
    "wid = len(forc_sim)\n",
    "tal = len(comp_sim)\n",
    "ndec = 1\n",
    "config_num = 1\n",
    "x = np.arange(ndec)\n",
    "inc = np.floor(ndec/config_num)\n",
    "if inc<1: inc=1\n",
    "xtic = np.arange(0, ndec,inc).tolist()\n",
    "xtic =[int(i) for i in xtic]\n",
    "xtics =[str(i+1) for i in xtic]\n",
    "labels =[\"D\"+i for i in xtics]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "plot1 = plt.figure(1, figsize = (20,20))\n",
    "\n",
    "for i, s in enumerate(comp_sim):\n",
    "    for j in np.arange(len(forc_sim)):\n",
    "        plot2 = plt.subplot(tal,wid,i*wid+j+1)\n",
    "      \n",
    "        data = rank_data[s].loc[default_name,:,:,'rank']\n",
    "        data = data.where(data==j+1)/(j+1)\n",
    "        data = data.sum(dim='hru')\n",
    "        data = data/data.sum(dim='var')*len(hrud)\n",
    "        data_Master = [0] * ndec\n",
    "        for jj, v in enumerate(reversed(forc_sim)):\n",
    "            plt.bar(height = data.loc[v], x = x, width = 1.0, color = col_forc[-jj-1], bottom = data_Master)\n",
    "            data_Master = data_Master+data.loc[v].values\n",
    "            \n",
    "        plt.title('('+letter[i]+letter[j]+') '+s[6:])\n",
    "        plt.xlim(-0.5,ndec-0.5)\n",
    "        plt.ylim(0,len(hrud))\n",
    "        plt.xticks(xtic, labels, fontsize = 5)\n",
    "        plt.yticks(np.arange(0, len(hrud)+.01, 1).tolist())\n",
    "        plt.tick_params(axis = \"x\", which = \"both\", bottom = False, top = False)\n",
    "        plt.xlabel(\"Decisions (\"+labels[0]+\"-D\"+str(ndec)+\")\", fontsize = 9)\n",
    "        plt.ylabel(\"Rank \"+str(j+1), fontsize = 9)\n",
    "\n",
    "        plt.subplots_adjust(hspace = .4)\n",
    "\n",
    "for j, v in enumerate(forc_sim):\n",
    "    plt.scatter([],[], color = col_forc[j], label = 'constant_' +  v)\n",
    "plt.figlegend(loc = 'center right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# Differences in Parameter Space and Model Configuration\n",
    "\n",
    "We can plot the summation patterns plotted earlier for each decision. A decision is a combination of Latin Hypercube parameter set and model configuration choice. We plot blocks of model configurations on the x-axis, so each configuration has 11 LHC parameter sets associated with it for 11 vertical bars with each. If the model configurations were run, there are 8 configurations. Based on the paper results, we might expect to see more differences across model configurations but not many across the parameter space. So, every block of 11 parameter sets will set itself somewhat apart. First, we just plot the default model configuration. The default parameters will show up as the first parameter set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup plots\n",
    "if lhc_prob==1 or lhc_config_prob==1:\n",
    "    wid = len(forc_sim)\n",
    "    tal = len(comp_sim)\n",
    "    ndec = 11\n",
    "    config_num = ndec/11\n",
    "    x = np.arange(ndec)\n",
    "    inc = np.floor(ndec/config_num)\n",
    "    if inc<1: inc=1\n",
    "    xtic = np.arange(0, ndec,inc).tolist()\n",
    "    xtic =[int(i) for i in xtic]\n",
    "    xtics =[str(i+1) for i in xtic]\n",
    "    labels =[\"D\"+i for i in xtics]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if lhc_prob==1 or lhc_config_prob==1:\n",
    "    plot1 = plt.figure(1, figsize = (20,20))\n",
    "\n",
    "    for i, s in enumerate(comp_sim):\n",
    "        for j in np.arange(len(forc_sim)):\n",
    "            plot2 = plt.subplot(tal,wid,i*wid+j+1)\n",
    "\n",
    "            data = rank_data[s].loc[lat_name,:,:,'rank']\n",
    "            data = data.where(data==j+1)/(j+1)\n",
    "            data = data.sum(dim='hru')\n",
    "            data = data/data.sum(dim='var')*len(hrud)\n",
    "            data_Master = [0] * ndec\n",
    "            for jj, v in enumerate(reversed(forc_sim)):\n",
    "                plt.bar(height = data.loc[:,v], x = x, width = 1.0, color = col_forc[-jj-1], bottom = data_Master)\n",
    "                data_Master = [m + n for m, n in zip(data_Master, data.loc[:,v])]\n",
    "\n",
    "            plt.title('('+letter[i]+letter[j]+') '+s[6:])\n",
    "            plt.xlim(-0.5,ndec-0.5)\n",
    "            plt.ylim(0,len(hrud))\n",
    "            plt.xticks(xtic, labels, fontsize = 5)\n",
    "            plt.yticks(np.arange(0, len(hrud)+.01, 1).tolist())\n",
    "            plt.tick_params(axis = \"x\", which = \"both\", bottom = False, top = False)\n",
    "            plt.xlabel(\"Decisions (\"+labels[0]+\"-D\"+str(ndec)+\")\", fontsize = 9)\n",
    "            plt.ylabel(\"Rank \"+str(j+1), fontsize = 9)\n",
    "\n",
    "            plt.subplots_adjust(hspace = .4)\n",
    "\n",
    "    for j, v in enumerate(forc_sim):\n",
    "        plt.scatter([],[], color = col_forc[j], label = 'constant_' +  v)\n",
    "    plt.figlegend(loc = 'center right')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "Now, if we calculated it, we can plot the full problem with all model configurations. The default will show up as block 4. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup plots\n",
    "if lhc_config_prob==1:\n",
    "    wid = len(forc_sim)\n",
    "    tal = len(comp_sim)\n",
    "    ndec = len(decision_set)\n",
    "    config_num = ndec/11\n",
    "    x = np.arange(ndec)\n",
    "    inc = np.floor(ndec/config_num)\n",
    "    if inc<1: inc=1\n",
    "    xtic = np.arange(0, ndec,inc).tolist()\n",
    "    xtic =[int(i) for i in xtic]\n",
    "    xtics =[str(i+1) for i in xtic]\n",
    "    labels =[\"D\"+i for i in xtics]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if lhc_config_prob==1:\n",
    "    plot1 = plt.figure(1, figsize = (20,20))\n",
    "\n",
    "    for i, s in enumerate(comp_sim):\n",
    "        for j in np.arange(len(forc_sim)):\n",
    "            plot2 = plt.subplot(tal,wid,i*wid+j+1)\n",
    "\n",
    "            data = rank_data[s].loc[:,:,:,'rank']\n",
    "            data = data.where(data==j+1)/(j+1)\n",
    "            data = data.sum(dim='hru')\n",
    "            data = data/data.sum(dim='var')*len(hrud)\n",
    "            data_Master = [0] * ndec\n",
    "            for jj, v in enumerate(reversed(forc_sim)):\n",
    "                plt.bar(height = data.loc[:,v], x = x, width = 1.0, color = col_forc[-jj-1], bottom = data_Master)\n",
    "                data_Master = [m + n for m, n in zip(data_Master, data.loc[:,v])]\n",
    "\n",
    "            plt.title('('+letter[i]+letter[j]+') '+s[6:])\n",
    "            plt.xlim(-0.5,ndec-0.5)\n",
    "            plt.ylim(0,len(hrud))\n",
    "            plt.xticks(xtic, labels, fontsize = 5)\n",
    "            plt.yticks(np.arange(0, len(hrud)+.01, 1).tolist())\n",
    "            plt.tick_params(axis = \"x\", which = \"both\", bottom = False, top = False)\n",
    "            plt.xlabel(\"Decisions (\"+labels[0]+\"-D\"+str(ndec)+\")\", fontsize = 9)\n",
    "            plt.ylabel(\"Rank \"+str(j+1), fontsize = 9)\n",
    "\n",
    "            plt.subplots_adjust(hspace = .4)\n",
    "\n",
    "    for j, v in enumerate(forc_sim):\n",
    "        plt.scatter([],[], color = col_forc[j], label = 'constant_' +  v)\n",
    "    plt.figlegend(loc = 'center right')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Should I make fig 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# Ranking Changes with other Model Configurations\n",
    "Let's compare this model output to other output from the other configurations, and see if we can change the rankings at all. \n",
    "This may be a good idea if, for example, windspeed is ranking as a variable contributing highly to model error and we do not have good estimates of windspeed. In that example, we would want to find a model configuration that ranked windspeed lower in error contribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "/glade/work/ashleyvb/CAMELs/regress_data/camels_attributes_v2.0.xlsx explains attributes in regression"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:miniconda3-pysumma]",
   "language": "python",
   "name": "conda-env-miniconda3-pysumma-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
