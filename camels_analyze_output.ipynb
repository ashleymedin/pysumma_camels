{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Effect of Forcings on CAMELs Simulations\n",
    "\n",
    "Now we can look at the output and see if there are any patterns across the variables or across basin characteristics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we load the imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import xarray as xr\n",
    "import bottleneck\n",
    "from scipy import stats\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.feature_selection import RFE, RFECV\n",
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### You will need to edit these paths to be your folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top = '/glade/work/ashleyvb'\n",
    "folder = top+'/CAMELs'\n",
    "folders = folder+'/summa_camels'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# Summary Statistics of Error on output\n",
    "Let's look at some error metrics by HRU.\n",
    "KGE means perfect agreement if it is 1, and <0 means the mean is a better guess. \n",
    "Bias means perfect aggreement if it is 0, and larger means larger error. \n",
    "All errors have 1's added so we don't divide by 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# truth data set\n",
    "sim_truth = xr.open_dataset(folders+'/output/merged_day/NLDAStruth_hru.nc')\n",
    "the_hru = np.array(sim_truth['hruId'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set forcings to hold at constant or MetSim and create dictionaries\n",
    "cm_vars= ['all','airpres','airtemp','LWRadAtm','pptrate','spechum','SWRadAtm','windspd']\n",
    "error_kind = ['bias','kge']\n",
    "est_kind = ['constant','metsim']\n",
    "seas_kind = ['YEAR','DJF','MAM','JJA','SON']\n",
    "#forcing, liquid water fluxes for the soil domain, turbulent heat transfer, snow, vegetation, derived \n",
    "forc_sim = np.delete(cm_vars,0)\n",
    "comp_sim=['scalarSurfaceRunoff','scalarAquiferBaseflow','scalarInfiltration','scalarRainPlusMelt','scalarSoilDrainage',\n",
    "          'scalarLatHeatTotal','scalarSenHeatTotal','scalarSnowSublimation',\n",
    "          'scalarSWE',\n",
    "          'scalarCanopyWat',\n",
    "          'scalarNetRadiation','scalarTotalET','scalarTotalRunoff','scalarTotalSoilWat']\n",
    "var_sim = np.concatenate([forc_sim, comp_sim])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# definitions for KGE computation\n",
    "def covariance(x,y,dims=None):\n",
    "    return xr.dot(x-x.mean(dims), y-y.mean(dims), dims=dims) / x.count(dims)\n",
    "\n",
    "def correlation(x,y,dims=None):\n",
    "    return covariance(x,y,dims) / (x.std(dims) * y.std(dims))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up xarray\n",
    "hrud = sim_truth['hru'] #indices here are 0 to number of basins\n",
    "shape = (len(hrud), len(cm_vars),len(est_kind), len(error_kind),len(seas_kind))\n",
    "dims = ('hru','var','estimation','error','season')\n",
    "coords = {'hru': hrud, 'var':cm_vars, 'estimation':est_kind, 'error':error_kind, 'season':seas_kind}\n",
    "error_data = xr.Dataset(coords=coords)\n",
    "for s in var_sim:\n",
    "    error_data[s] = xr.DataArray(data=np.full(shape, np.nan),\n",
    "                                 coords=coords, dims=dims,\n",
    "                                 name=s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "Now run the actual computations on KGE. This takes 35 min using all 671 basins. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "truth0 = sim_truth.drop_vars('hruId').load()\n",
    "for v in cm_vars:\n",
    "    for c in est_kind:     \n",
    "        sim0 = xr.open_dataset(folders+'/output/merged_day/NLDAS' + c + '_' + v +'_hru.nc')\n",
    "        sim0 = sim0.drop_vars('hruId').load()\n",
    "        for i, t in enumerate(seas_kind):     \n",
    "            if i==0: \n",
    "                truth = truth0\n",
    "                sim = sim0\n",
    "            if i>0: \n",
    "                truth = truth0.sel(time=truth0['time.season']==t)\n",
    "                sim = sim0.sel(time=sim0['time.season']==t)\n",
    "                \n",
    "            r = sim.mean(dim='time') #to set up xarray since xr.dot not supported on dataset and have to do loop\n",
    "            for s in var_sim:         \n",
    "                r[s] = correlation(sim[s],truth[s],dims='time')\n",
    "            # KGE value for each hru, add 1 so no nan\n",
    "            ds = 1 - np.sqrt( np.square(r-1) \n",
    "                + np.square( (sim.std(dim='time')+1)/(truth.std(dim='time')+1) - 1) \n",
    "                + np.square( (sim.mean(dim='time')+1)/(truth.mean(dim='time')+1) - 1) )\n",
    "            ds0 = ds.load()\n",
    "            # bias value for each hru, add 1 so no nan\n",
    "            ds = np.abs(sim-truth)/(truth+1) \n",
    "            ds1 = ds.mean(dim='time').load()\n",
    "            for s in var_sim:\n",
    "                error_data[s].loc[:,v,c,'kge',t]  = ds0[s]\n",
    "                error_data[s].loc[:,v,c,'bias',t] = ds1[s]\n",
    "    print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change coordinates and save incase hangs up\n",
    "error_data = error_data.assign_coords(hru=sim_truth['hruId'])\n",
    "error_data.to_netcdf(folder+'/regress_data/error_data.nc') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "KGE does not need to be normalized. We plot the HRU error as stack of values, with no error plotting as a height of 1 for that color. Values less than 0 are plotted as 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#error_data =  xr.open_dataset(folder+'/regress_data/error_data.nc') #read this incase hangs up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup plots\n",
    "x = np.arange(len(hrud))\n",
    "col_vars = ['gray','y','r','g','orange','c','m','b']\n",
    "letter = list('ABCDEFGHIJKLMNOPQRSTUVWXYZ')\n",
    "wid = ceil(len(var_sim)/3)\n",
    "inc = floor(len(hrud)/10)\n",
    "if inc<1: inc=1\n",
    "xtic = np.arange(0, len(hrud),inc).tolist()\n",
    "xtic =[int(i) for i in xtic]\n",
    "xtics =[str(i+1) for i in xtic]\n",
    "labels =[\"V\"+i for i in xtics]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "Do the plotting. This takes about 8 min with all 671 basins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Just plotting all seasons. Maybe add winter (ind 1), and summer (ind 3) if you want to see more detail. \n",
    "ind = [0]#,1,3]\n",
    "seas_kind0 = [ seas_kind[i] for i in ind]\n",
    "for c in est_kind:      \n",
    "    for t in seas_kind0:     \n",
    "        plot1 = plt.figure(1, figsize = (20,10))\n",
    "\n",
    "        for i, s in enumerate(var_sim):\n",
    "            data0 = error_data[s].loc[:,:,c,'kge',t]\n",
    "            data = data0.where(data0>0,0) #make the negative values be 0\n",
    "            data_Master = [0] * len(hrud)\n",
    "    \n",
    "            plot2 = plt.subplot(3,wid,i+1)\n",
    "            for j, v in enumerate(cm_vars):\n",
    "                plt.bar(height = data.loc[:,v], x = x, width = 1.0, color = col_vars[j], bottom = data_Master)\n",
    "                #data_Master = [m + n for m, n in zip(data_Master, data.loc[:,v])]\n",
    "                data_Master = [j+1] * len(hrud)\n",
    "         \n",
    "            plt.title('('+letter[i]+') '+s)\n",
    "            plt.ylim(0,len(cm_vars))\n",
    "            plt.xticks(xtic, labels, fontsize = 5)\n",
    "            plt.yticks(np.arange(0, len(cm_vars)+.05, 1).tolist())\n",
    "            plt.tick_params(axis = \"x\", which = \"both\", bottom = False, top = False)\n",
    "            plt.xlabel(\"CAMELS basin (\"+labels[0]+\"-\"+labels[-1]+\")\", fontsize = 9)\n",
    "            plt.ylabel(\"KGE\", fontsize = 9)\n",
    "\n",
    "        plt.subplots_adjust(hspace = .4)\n",
    "\n",
    "        for j, v in enumerate(cm_vars):\n",
    "            plt.scatter([],[], color = col_vars[j], label = t + '_NLDAS_' + c + '_' + v)\n",
    "        plt.figlegend(loc = 'lower right')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "We see that the pptrate and air pressure would be better off constant than at MetSim values (thiner orange and yellow layers in the MetSim plots), but that the air pressure does not matter in the variable calculation (except simulation of air pressure itself). Air temperature has less error in MetSim. \n",
    "By season, there is more error in the winter in both Metsim and Constant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# Best run with 671 BASINS: Correlations of Error and Basin Attributes\n",
    "We look at the basin attributes to see if there are any patterns with the error sizes. \n",
    "This code will run with fewer than 671 basins, but stronger relationships between error and basin attributes can be surmised if you look at all the basins. \n",
    "We use the Kendall non-parametric correlation based on ranks, so that error magnitude (that is likely more affected by calibrated or not calibrated parameters) is not a factor. \n",
    "The attribute file that SUMMA uses does not have many continuous variables in it, so we use the raw attribute data that would have been used to derive the SUMMA attribute file and the parameters.\n",
    "TEST Budyko of each setup??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start wih attribute data used by SUMMA. It does not have many values and a bunch of them are indices.\n",
    "attrib = xr.open_dataset(folders+'/settings.v1/attributes.nc')\n",
    "print(attrib)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO LOOK AT THE 671 WITH THE SAVED STUFF\n",
    "attrib = xr.open_dataset(folders+'/settings.v1/attributes.camels.v2.nc')\n",
    "sim_truth = xr.open_dataset(folders+'/output/merged_dayall671/NLDAStruth_hru.nc')\n",
    "hrud = sim_truth['hru'] #indices here are 0 to number of basins\n",
    "the_hru = np.array(sim_truth['hruId'])\n",
    "error_data =  xr.open_dataset(folder+'/regress_data/error_dataall671.nc') #read this incase hangs up\n",
    "cm_vars= ['all','airpres','airtemp','LWRadAtm','pptrate','spechum','SWRadAtm','windspd']\n",
    "error_kind = ['bias','kge']\n",
    "est_kind = ['constant','metsim']\n",
    "seas_kind = ['YEAR','DJF','MAM','JJA','SON']\n",
    "forc_sim = np.delete(cm_vars,0)\n",
    "comp_sim=['scalarSurfaceRunoff','scalarAquiferBaseflow','scalarInfiltration','scalarRainPlusMelt','scalarSoilDrainage',\n",
    "          'scalarLatHeatTotal','scalarSenHeatTotal','scalarSnowSublimation',\n",
    "          'scalarSWE',\n",
    "          'scalarCanopyWat',\n",
    "          'scalarNetRadiation','scalarTotalET','scalarTotalRunoff','scalarTotalSoilWat']\n",
    "var_sim = np.concatenate([forc_sim, comp_sim])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to get all HRU ids\n",
    "file_list = []\n",
    "filelist = open(folders+'/settings.v1/forcingFileList.1hr.txt', 'r')\n",
    "for lineNumber, line in enumerate (filelist):\n",
    "   file_list.append(folders+'/forcing/1hr/'+line.strip(\"'\\n\"))\n",
    "filelist.close()\n",
    "extra_vars0 = xr.open_dataset(file_list[0]) \n",
    "extra_vars0 = extra_vars0.assign_coords(hru=extra_vars0['hruId'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variables to regress, take only floats\n",
    "lr_attrib = attrib.get(['hruId']) \n",
    "lr_attrib = lr_attrib.assign_coords(hru=lr_attrib['hruId'])\n",
    "file_name0 = ['clim','geol','hydro','soil','topo','vege']\n",
    "file_name = file_name0.copy()\n",
    "n_attrib = file_name\n",
    "for i, f in enumerate(file_name):\n",
    "    df = pd.read_csv(folder+'/regress_data/camels_'+f+'.txt',delimiter=';')\n",
    "    df['hru'] = range(0,671)\n",
    "    xr_tmp = df.set_index(['hru']).to_xarray()\n",
    "    xr_tmp = xr_tmp.assign_coords(hru=extra_vars0['hruId'])\n",
    "    xr_tmp = xr_tmp.sel(hru=the_hru)\n",
    "    xr_att = xr_tmp.drop_vars([ var for var in xr_tmp.variables if not 'float64'==xr_tmp[var].dtype ])\n",
    "    xr_att = (xr_att - xr_att.mean(dim='hru', skipna=True))/xr_att.std(dim='hru', skipna=True) #normalize\n",
    "    if i==0: n_attrib[i]= len(xr_att.variables)-1\n",
    "    if i>0: n_attrib[i]= len(xr_att.variables)+n_attrib[i-1]\n",
    "    lr_attrib =xr.merge([lr_attrib,xr_att])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add hruId as coordinates and print results\n",
    "lr_attrib = lr_attrib.drop('hruId').load()\n",
    "attrib_kind = list(lr_attrib.variables.keys())\n",
    "attrib_kind.remove('hru')\n",
    "print(n_attrib)\n",
    "print(attrib_kind)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "Now run the regressions and plot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up xarray, only do off KGE\n",
    "attrib_num = ['clim','geol','hydro','soil','topo','vege']\n",
    "shape = (len(attrib_kind), len(est_kind), len(seas_kind))\n",
    "dims = ('attrib','estimation','season')\n",
    "coords = {'attrib': attrib_kind, 'estimation':est_kind,'season':seas_kind}\n",
    "corr_data = xr.Dataset(coords=coords)\n",
    "for v in cm_vars:\n",
    "    corr_data[v] = xr.DataArray(data=np.full(shape, np.nan),\n",
    "                                 coords=coords, dims=dims,\n",
    "                                 name=v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for fewer basins, get rid of significance\n",
    "PT = 0.01\n",
    "# get number of HRUs\n",
    "if len(the_hru) <25: PT = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def r_cor(x,y, pthres = PT, direction = False):\n",
    "    \"\"\"\n",
    "    Uses the scipy stats module to calculate a Kendall correlation test\n",
    "    :pthres: Significance of the underlying test\n",
    "    :direction: output only direction as output (-1 & 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Check NA values\n",
    "    nas = np.logical_or(np.isnan(x), np.isnan(y))\n",
    "    if len(x[~nas]) < 10: # If fewer than 10 data return nan\n",
    "        return np.nan\n",
    "    if np.linalg.norm(x[~nas] - np.mean(x[~nas])) < 1e-13 * abs(np.mean(x[~nas])): #near constant attibute\n",
    "         return np.nan\n",
    "    if np.linalg.norm(y[~nas] - np.mean(y[~nas])) < 1e-13 * abs(np.mean(y[~nas])): #near constant error\n",
    "         return np.nan\n",
    "       \n",
    "    \n",
    "    # Run the kendalltau test\n",
    "    #stat, p_value = stats.kendalltau(x[~nas], y[~nas])\n",
    "    # Run the spearmanr test\n",
    "    stat, p_value = stats.spearmanr(x[~nas], y[~nas])\n",
    "    # Run the pearsonr test\n",
    "    #stat, p_value = stats.pearsonr(x[~nas], y[~nas])\n",
    "    \n",
    "    # Criterium to return results in case of Significance\n",
    "    if p_value < pthres:\n",
    "        # Check direction\n",
    "        if direction:\n",
    "            if stat < 0:\n",
    "                return -1\n",
    "            elif stat > 0:\n",
    "                return 1\n",
    "        else:\n",
    "            return stat\n",
    "    else:\n",
    "      return 0  \n",
    "\n",
    "# The function we are going to use for applying our statistics test\n",
    "def rank_correlation(x,y):\n",
    "    return xr.apply_ufunc(\n",
    "        r_cor, x , y\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <br>\n",
    "Calculate the correlations. This takes about a minute. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ds = error_data.get(comp_sim)\n",
    "for a in attrib_kind:\n",
    "    ds0 = lr_attrib[a]\n",
    "    for c in est_kind:\n",
    "        for t in seas_kind:\n",
    "            for v in cm_vars:\n",
    "                ds1 = ds.loc[dict(var = v,estimation=c,error = 'kge',season=t)]\n",
    "                ds1 = ds1.where(ds1>-1,-1) #make the very negative values be -1\n",
    "                ds1 = sum(d for d in ds1.data_vars.values())\n",
    "                value = rank_correlation(ds0.values, ds1.values)\n",
    "                corr_data[v].loc[a,c,t] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup plots\n",
    "x = np.arange(len(attrib_kind))\n",
    "col_vars = ['gray','y','r','g','orange','c','m','b']\n",
    "letter = list('ABCDEFGHIJKLMNOPQRSTUVWXYZ')\n",
    "xtic = n_attrib\n",
    "labels=file_name0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "Do the plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Just plotting all seasons. Maybe add winter (ind 1), and summer (ind 3) if you want to see more detail. \n",
    "ind = [0]#,1,3]\n",
    "seas_kind0 = [ seas_kind[i] for i in ind]\n",
    "for c in est_kind:      \n",
    "    for t in seas_kind0:     \n",
    "        plot1 = plt.figure(1, figsize = (7,7))\n",
    "        data0 = corr_data.loc[dict(estimation=c,season=t)]\n",
    "        #data = abs(data0)\n",
    "        data = data0\n",
    "        data_Master = [0] * len(attrib_kind)  \n",
    "        for j, v in enumerate(cm_vars):\n",
    "            plt.bar(height = data[v], x = x, width = 1.0, color = col_vars[j], bottom = data_Master)\n",
    "            #data_Master = [m + n for m, n in zip(data_Master, data.loc[:,v])]\n",
    "            data_Master = [j+1] * len(attrib_kind)\n",
    "        \n",
    "        plt.title('Sum Over Simulated Variables')\n",
    "        plt.xticks(xtic, labels, fontsize = 6)\n",
    "        #plt.ylim(0,(len(cm_vars)/1.8))\n",
    "        #plt.yticks(np.arange(0, (len(cm_vars)/2.0)+.55, 0.5).tolist())\n",
    "        plt.ylim(0,len(cm_vars))\n",
    "        plt.yticks(np.arange(-0.5, len(cm_vars)+.05, 1).tolist())\n",
    "        plt.tick_params(axis = \"x\", which = \"both\", bottom = False, top = False)\n",
    "        plt.xlabel(\"CAMELS Attrib (a1-a52)\", fontsize = 9)\n",
    "        plt.ylabel(\"Spearman Correlation with KGE\", fontsize = 9)\n",
    "\n",
    "        for j, v in enumerate(cm_vars):\n",
    "            plt.scatter([],[], color = col_vars[j], label = t + '_NLDAS_' + c + '_' + v)\n",
    "        plt.figlegend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up xarray, only do off KGE\n",
    "# Just do all seasons. Maybe add winter (ind 1), and summer (ind 3) if you want to see more detail. \n",
    "ind = [0]#,1,3]\n",
    "seas_kind0 = [ seas_kind[i] for i in ind]\n",
    "\n",
    "pick_top = 5 #len(attrib_kind)\n",
    "pick_coord = range(0, pick_top)\n",
    "\n",
    "rank_coord = ['raw','rank']\n",
    "\n",
    "shape = (len(est_kind), len(seas_kind0),pick_top, len(rank_coord))\n",
    "\n",
    "dims = ('estimation','season','selected','rank')\n",
    "coords = {'estimation':est_kind,'season':seas_kind0,'selected':pick_coord, 'rank':rank_coord}\n",
    "rfe_data = xr.Dataset(coords=coords)\n",
    "for v in cm_vars:\n",
    "    rfe_data[v] = xr.DataArray(data=np.full(shape, np.nan),\n",
    "                                 coords=coords, dims=dims,\n",
    "                                 name=v)\n",
    "rfe_data = rfe_data.astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "This takes about a 10 seconds with all 671 subbasins. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ds = error_data.get(comp_sim)\n",
    "ds =ds.fillna(0) # So don't add to total KGE\n",
    "ds0 = lr_attrib\n",
    "# Make the nan's the means of each attribute variable (imputer takes mean over column so need to transpose first)\n",
    "imputer = SimpleImputer()\n",
    "X = ds0.to_array().transpose()\n",
    "X_imputed = imputer.fit_transform(X)\n",
    "length = pick_top\n",
    "for c in est_kind:\n",
    "    for t in seas_kind0:\n",
    "        ds1 = ds.loc[dict(estimation=c,error = 'kge',season=t)] # by KGE, all vars\n",
    "        ds2 = ds1.rank(dim='var')/len(hrud) #by rank         \n",
    "        ds1 = ds1.where(ds1>-1,-1) #make the very negative values be -1            \n",
    "        for i in rank_coord: #just do rank\n",
    "            if (i=='raw'): ds4 = ds1 # by actual value\n",
    "            if (i=='rank'): ds4 = ds2 # by rank\n",
    "            ds4 = sum(d for d in ds4.data_vars.values())\n",
    "            for v in cm_vars:\n",
    "                ds3 = ds4.loc[:,v]\n",
    "                # Choose top some in MLE\n",
    "                rfe = RFE(estimator=LinearRegression(), n_features_to_select=pick_top)\n",
    "                # Automatically choose the number of features, this gives lower R2 values\n",
    "                #rfe = RFECV(estimator=LinearRegression()) \n",
    "\n",
    "                # Do fit\n",
    "                rfe.fit(X_imputed, ds3.values)\n",
    "                r2 = rfe.score(X_imputed, ds3.values)\n",
    "                adjusted_r2 = 1 - (1-rfe.score(X_imputed, ds3.values))*(len(ds3.values)-1)/(len(ds3.values)-X_imputed.shape[1]-1)\n",
    "                print(c,t,i,v,len(np.array(attrib_kind)[rfe.support_]),r2, adjusted_r2)\n",
    "                if (len(np.array(attrib_kind)[rfe.support_])<5): length = len(np.array(attrib_kind)[rfe.support_])\n",
    "                rfe_data[v].loc[c,t,range(0,length),i] = np.array(attrib_kind)[rfe.support_][range(0,length)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rfe_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(np.array(attrib_kind)[rfe.support_])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:miniconda3-pysumma]",
   "language": "python",
   "name": "conda-env-miniconda3-pysumma-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
