{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating forcing datasets for pySUMMA\n",
    "\n",
    "Here we will put together forcing data setups for CAMELs.\n",
    "This requires hourly forcings of\n",
    " - temperature (K)\n",
    " - precipitation (kg/m^2/s^1)\n",
    " - shortwave radiation (W/m^2)\n",
    " - longwave radiation (W/m^2)\n",
    " - specific humidity (g/g)\n",
    " - air pressure (Pa)\n",
    " - wind speed (m/s)\n",
    " \n",
    "NLDAS hourly forcings are considered to be \"truth\". \n",
    "We take NLDAS hourly forcings and give them their mean daily values for each variable in turn (or all variables). \n",
    "Then we take NLDAS daily forcings of maximum and minimum temperature, total precipitation, and mean windspeed; and redistribute them to hourly with calculation of the other variables using the MetSim python package. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start, the first thing we need to do is make sure SUMMA and pysumma are installed. Commands:\n",
    "* `cd /path/to/home`\n",
    "* `github clone https://github.com/ashleymedin/summa.git`\n",
    "* `cd summa`\n",
    "* `github checkout develop`\n",
    "* `cd build`\n",
    "\n",
    "In the makefile, at a minimum you will need to set the following:\n",
    "* F_MASTER         - top level summa directory\n",
    "* FC               - compiler suite\n",
    "* FC_EXE           - compiler executable\n",
    "* INCLUDES         - path to include files\n",
    "* LIBRARIES        - path to and libraries to include\n",
    "\n",
    "This is setup in the repo for Cheyenne under Makefile_cheyenne, which you can copy and then build. Commands:\n",
    "* `cp Makefile_cheyenne my_makefile`\n",
    "* `./build_summa`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, get an Anaconda Python environment set up and install the environment there as follows. \n",
    "\n",
    "Check the operating system:\n",
    "* `cd /path/to/home`\n",
    "* `uname -m`\n",
    "Then get Miniconda for that system. For Linux 64 like Cheyenne, the command is:\n",
    "* `wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh`\n",
    "* `chmod 777 Miniconda3-latest-Linux-x86_64.sh`\n",
    "* `./Miniconda3-latest-Linux-x86_64.sh`\n",
    "Close and then re-open your terminal window.\n",
    "\n",
    "Now install the environment with conda the extra packages, with commands:\n",
    "* `cd /path/to/home`\n",
    "* `git clone https://github.com/ashleymedin/pysumma.git`\n",
    "* `cd pysumma`\n",
    "* `git checkout develop`\n",
    "* `conda env create -f environment.yml`\n",
    "\n",
    "This will make a python environment that you can activate and deactivate. \n",
    "Activate it-- this will install metsim, some plotting tools, and the dask tools for good measure: \n",
    "* `conda activate pysumma` \n",
    "\n",
    "Install it as a kernel in your Jupyter environments:\n",
    "* `python -m ipykernel install --user --name=pysumma`\n",
    "\n",
    "Then install pySUMMA\n",
    "* `python setup.py develop`\n",
    "\n",
    "Make sure to run `git pull origin`inside `/summa` and `/pysumma` periodically to keep things up to date. You need to rebuild SUMMA if there are changes. You *do not* need to run `setup.py` again if you used develop. \n",
    "If you want to deactivate, run `conda deactivate`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "Now, let's make the inputs for pySumma. First we check that we loaded correct environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conda list metsim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "Then we load some standard imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import geoviews as gv\n",
    "import geopandas as gpd\n",
    "import holoviews as hv\n",
    "import xarray as xr\n",
    "import ogr\n",
    "import cartopy\n",
    "\n",
    "hv.notebook_extension('bokeh')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### You will need to edit these paths to be your folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some folder places\n",
    "top = '/glade/work/ashleyvb'\n",
    "folder = top+'/CAMELs'\n",
    "folders = folder+'/summa_camels'\n",
    "shapefile = folder+'/basin_set_full_res_simple/HCDN_nhru_final_671.shp'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# Our study location: the CAMELs basins\n",
    "\n",
    "We are using MetSim on unstructured mesh arrangements as well as the structured latitude-longitude grid that we ran in the command line examples.\n",
    "We will use a setup which consists of 671 \"hydrologic response units\" or HRU.\n",
    "A HRU typically delineates a watershed by topography, soil type, land use, or other defining feature.\n",
    "Here, each CAMELs basin is run as a lumped model, so each one is an HRU.\n",
    "\n",
    "Let's take a look. \n",
    "Because this is an unstructured mesh we will be defining the mesh elements by their respective basin identifiers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list = []\n",
    "filelist = open(folders+'/settings.v1/forcingFileList.1hr.txt', 'r')\n",
    "for lineNumber, line in enumerate (filelist):\n",
    "   file_list.append(folders+'/forcing/1hr/'+line.strip(\"'\\n\"))\n",
    "filelist.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some things get dumped with open_mfdataset, so keep them here\n",
    "extra_vars0 = xr.open_dataset(file_list[0]) \n",
    "extra_vars0 = extra_vars0.assign_coords(hru=extra_vars0['hruId'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### THIS IS IMPORTANT! Select here what basins you want to run. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All HRUs, select out all or some to look at, assuming have GRUs same as HRUs here\n",
    "#the_hru = np.array(extra_vars0['hruId']) # run all\n",
    "the_hru = np.array([1413500, 7261000, 9223000, 12488500])\n",
    "the_gru = the_hru\n",
    "extra_vars = extra_vars0.sel(hru=the_hru)\n",
    "print(the_hru)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf = gpd.read_file(shapefile)\n",
    "shapes = cartopy.io.shapereader.Reader(shapefile)\n",
    "list(shapes.records())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the data from an xarray dataset to a pandas dataframe \n",
    "out_df = extra_vars0['hru']\n",
    "out_df = out_df.to_dataframe()\n",
    "# Make sure we have some metadata to join with the shapefile\n",
    "out_df['hru_id'] = gdf['hru_id'].values\n",
    "#search for the ones with desired records\n",
    "find_rec = out_df.loc[the_hru,:]['hru_id']\n",
    "#look at attributes\n",
    "desired_shapes = []\n",
    "for i in find_rec:\n",
    "    for s in shapes.records():\n",
    "        if s.attributes['hru_id'] == i :\n",
    "            desired_shapes.append(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create backgound\n",
    "mapp =gv.tile_sources.StamenTerrainRetina.opts(width=900, height=500)\n",
    "# Create the shape plot\n",
    "poly = gv.Shape.from_records(shapes.records(), out_df, index=['hru_id'], on='hru_id',crs=cartopy.crs.PlateCarree())\n",
    "poly2 = gv.Shape.from_records(desired_shapes,out_df.loc[the_hru,:],on='hru_id', crs=cartopy.crs.PlateCarree())\n",
    "poly = poly.opts(cmap='plasma', tools=['hover'], colorbar=True, alpha=0.8)\n",
    "poly2 = poly2.opts(fill_color='cyan', line_color='cyan', alpha=0.8)\n",
    "# Plot\n",
    "mapp*poly*poly2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basins are colored by index values and are hoverable for index values and IDs. Selected basins are higlighted in cyan."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# Make SUMMA files the correct HRUs\n",
    "We have to select out only the HRUs of the basins we are using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attributes \n",
    "attrib = xr.open_dataset(folders+'/settings.v1/attributes.camels.v2.nc')\n",
    "attrib = attrib.assign_coords(hru=attrib['hruId'])\n",
    "attrib = attrib.assign_coords(gru=attrib['gruId'])\n",
    "gg = attrib['gruId'] # save because gruId was missing from the parameter file\n",
    "attrib = attrib.sel(hru=the_hru)\n",
    "attrib = attrib.sel(gru=the_gru)\n",
    "attrib = attrib.drop(['hru','gru']) #summa doesn't like these to have coordinates\n",
    "attrib.to_netcdf(folders+'/settings.v1/attributes.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "param = xr.open_dataset(folders+'/settings.v1/trialParams.camels.Oct2020.nc')\n",
    "param = param.assign_coords(hru=param['hruId'])\n",
    "param = param.assign_coords(gru=gg) # there should be a gruId in here, but there wasn't\n",
    "param = param.sel(hru=the_hru)\n",
    "param = param.sel(gru=the_gru)\n",
    "param = param.drop(['hru','gru']) #summa doesn't like these to have coordinates\n",
    "param.to_netcdf(folders+'/settings.v1/parameters.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "Lastly we will need to make the constant initial conditions file.\n",
    "\n",
    "### You will need to edit these paths to be your folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! cd /glade/work/ashleyvb/CAMELs/summa_camels/settings.v1; source activate /glade/work/ashleyvb/miniconda3/envs/pysumma; python gen_coldstate.py attributes.nc init_cond.nc int"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# Forcing files of NLDAS with Constant Daily Values\n",
    "Here we make the NLDAS data hourly values into constant 24 hourly values for each forcing variable. \n",
    "We need to make these 24 hours represent a local day, so local time zones, such that later calculations on days work. \n",
    "Since SUMMA will impose that shortwave radiation is 0 when the sun is below the horizon, we distribute the constant shortwave radiation only during the daylight hours.\n",
    "Other changes inside SUMMA are that specific humidity will be lowered in order that relative humidity does not exceed 100%, and tiny windspeeds will be elimated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "Merge everything in the folder along time. This takes ~2 minutes.\n",
    "We set 'minimal' so it won't add the time dimension to the variables that don't have it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "truth = xr.open_mfdataset(folders+'/forcing/1hr/*.nc',data_vars='minimal',combine='by_coords').load()\n",
    "truth = truth.assign_coords(hru=truth['hruId'])\n",
    "truth = truth.sel(hru=the_hru)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix time encoding to be the same since the merge drops it\n",
    "truth.time.encoding = extra_vars.time.encoding\n",
    "# Also fix the variables that have no time dimension as they may get merged incorrectly by open_mfdataset\n",
    "truth['data_step'] = extra_vars['data_step']\n",
    "# Pull out HRUs\n",
    "truth = truth.assign_coords(hru=truth['hruId'])\n",
    "truth = truth.sel(hru=the_hru) # none of these in in here, so maybe a dummy file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "Write this file for pySUMMA forcing and save the forcing file name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save\n",
    "t0 = truth['time'].values[0] \n",
    "tl = truth['time'].values[-1]\n",
    "t0_s = pd.to_datetime(str(t0))\n",
    "t0_sf =t0_s.strftime('%Y%m%d')\n",
    "tl_s = pd.to_datetime(str(tl))\n",
    "tl_sf =tl_s.strftime('%Y%m%d')\n",
    "ffname ='NLDAStruth_' + t0_sf +'-' + tl_sf +'.nc'\n",
    "truth.to_netcdf(folders+'/forcing/truth/'+ffname)\n",
    "fflistname = folders+'/settings.v1/forcingFileList.truth.txt' \n",
    "file =open(fflistname,\"w\")\n",
    "file.write(ffname)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "Now we can make the daily data, first shifting to local time zones with longitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate out variables with no time dimension and add offset\n",
    "ds = truth\n",
    "ds_withtime = ds.drop_vars([ var for var in ds.variables if not 'time' in ds[var].dims ])\n",
    "ds_timeless = ds.drop_vars([ var for var in ds.variables if     'time' in ds[var].dims ])\n",
    "DEG_PER_REV = 360.0       # Number of degrees in full revolution\n",
    "HRS_PER_DAY = 24\n",
    "offset = (attrib['longitude'] / DEG_PER_REV) * HRS_PER_DAY\n",
    "offset = offset.astype(int)\n",
    "ds_withtime['offset'] = offset\n",
    "ds_withtime = ds_withtime.assign_coords(hru=ds_timeless['hruId'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "Here are the time zone changes. \n",
    "This takes about a minute using all 671 basins; a subset of basins should be shorter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for t in np.unique(offset.data):\n",
    "    ds = ds_withtime.where(offset==t,drop=True)\n",
    "    ds = ds.shift(time=t)\n",
    "    if t==np.unique(offset.data)[0]: ds0 = ds\n",
    "    if t>np.unique(offset.data)[0]: ds0 = xr.concat([ds0,ds],dim='hru')\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort back, and drop offset, keep airtemp for later calculations\n",
    "ds_withtime = ds0.sortby('hru')\n",
    "ds_withtime = ds_withtime.drop_vars('offset') \n",
    "air24 = ds_withtime.get('airtemp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "Downsample hourly time-series data to daily data. \n",
    "This takes about a 30 seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "truth24 = xr.merge([ds_timeless, ds_withtime.resample(time='1D').mean()]).load()\n",
    "# Fix time encoding to be the same since the merge drops it\n",
    "truth24.time.encoding = extra_vars.time.encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "Then we upsample this back to hourly data for constant daily values. \n",
    "We need to undo the time zone changes after we upsample. \n",
    "This whole process takes about a minute using all 671 basins; a subset of basins should be shorter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a fake day of data so upsamples until the end\n",
    "day_fake = truth24.isel(time=-1)['time']+np.timedelta64(1,'D')\n",
    "add_fake = truth24.isel(time=-1)\n",
    "add_fake['time'] = day_fake\n",
    "truth24_add = xr.concat([truth24, add_fake], dim='time',data_vars='minimal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Again we have to separate out variables with no time dimension.\n",
    "ds = truth24_add\n",
    "ds_withtime = ds.drop_vars([ var for var in ds.variables if not 'time' in ds[var].dims ])\n",
    "ds_timeless = ds.drop_vars([ var for var in ds.variables if     'time' in ds[var].dims ])\n",
    "ds_withtime = ds_withtime.resample(time='1H').ffill()\n",
    "ds_withtime['offset'] = offset\n",
    "ds_withtime = ds_withtime.assign_coords(hru=ds_timeless['hruId'])\n",
    "for t in np.unique(offset.data):\n",
    "    ds = ds_withtime.where(offset==t,drop=True)\n",
    "    ds = ds.shift(time=-t)\n",
    "    if t==np.unique(offset.data)[0]: ds0 = ds\n",
    "    if t>np.unique(offset.data)[0]: ds0 = xr.concat([ds0,ds],dim='hru')\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort back, and drop offset, and merge\n",
    "ds_withtime = ds0.sortby('hru')\n",
    "ds_withtime = ds_withtime.drop_vars('offset') \n",
    "constant_all = xr.merge([ds_timeless, ds_withtime])\n",
    "constant_all = constant_all.sel(hru=the_hru) #put back in original order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take extra day off\n",
    "constant_all = constant_all.isel(time=slice(0,-1))\n",
    "# Fix time encoding to be the same since the merge drops it\n",
    "constant_all.time.encoding = extra_vars.time.encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Scale Constant SW Radiation\n",
    "Edit the constant daily shortwave radiation so that energy is the same in the \"truth\" when pySUMMA makes the shortwave radiation zero during the day.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find where 0's shoud be based on original NLDAS data\n",
    "zero_one = truth['SWRadAtm']/truth['SWRadAtm']\n",
    "zero_one = zero_one.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find how much too small shortwave is each day if we use these 0's\n",
    "swr0 = zero_one*constant_all['SWRadAtm']\n",
    "div = swr0.resample(time='1D').mean()/constant_all['SWRadAtm'].resample(time='1D').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upsample, again add a fake day of data so upsamples until the end\n",
    "add_fake = div.isel(time=-1)\n",
    "add_fake['time'] = day_fake\n",
    "div_add = xr.concat([div, add_fake], dim='time')\n",
    "div_add = div_add.resample(time='1H').ffill()\n",
    "\n",
    "# Take extra day off\n",
    "div = div_add.isel(time=slice(0,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally add back in this constant shortwave radiation\n",
    "swr0 = swr0/div\n",
    "constant_all['SWRadAtm']=swr0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Files with Only One Variable Constant\n",
    "Now make files with only one variable held at daily means and save forcing file names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = constant_all['time'].values[0] \n",
    "tl = constant_all['time'].values[-1]\n",
    "t0_s = pd.to_datetime(str(t0))\n",
    "t0_sf =t0_s.strftime('%Y%m%d')\n",
    "tl_s = pd.to_datetime(str(tl))\n",
    "tl_sf =tl_s.strftime('%Y%m%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "constant_vars=['airpres','airtemp','LWRadAtm','pptrate','spechum','SWRadAtm','windspd']\n",
    "for v in constant_vars:\n",
    "    constant_one = truth.copy()\n",
    "    constant_one[v]= constant_all[v]\n",
    "    ffname ='NLDASconstant_' + v +'_forcing_' + t0_sf +'-' + tl_sf +'.nc'\n",
    "    constant_one.to_netcdf(folders+'/forcing/constant/'+ffname)\n",
    "    fflistname = folders+'/settings.v1/forcingFileList.constant_' + v + '.txt' \n",
    "    file =open(fflistname,\"w\")\n",
    "    file.write(ffname)\n",
    "    file.close()\n",
    "    print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# Check Files\n",
    "\n",
    "To make sure things look how we want, we plot the constant dataset against the NLDAS \"truth\" dataset, and plot the cumulative variables to see how errors are compounding. \n",
    "We plot one HRU (the first one) for 2 months."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot hourly\n",
    "fig, axes = plt.subplots(nrows=7, ncols=1, figsize=(20, 20))\n",
    "axes = axes.flatten()\n",
    "axes[0].set_title('Hourly')\n",
    "\n",
    "unit_str = ['($ ^o K$)', '($kg/m/s$)', '($W/m^2$)','($w/m^2$)','($g/g$)','($Pa$)', '($m/s$)',]\n",
    "\n",
    "variables = list(ms_pysum.variables.keys())\n",
    "dims = list(ms_out.dims.keys())\n",
    "[variables.remove(d) for d in dims]\n",
    "\n",
    "start =  24*7*30 \n",
    "stop = start + 2*30*24 \n",
    "#truth starts 90 days earlier\n",
    "truth_plt = truth.isel(hru=0, time=slice(start+90*24, stop+90*24))\n",
    "constant_all_plt = constant_all.isel(hru=0, time=slice(start+90*24, stop+90*24))\n",
    "\n",
    "for idx, var in enumerate(variables[0:7]):\n",
    "    truth_plt[var].plot(ax=axes[idx],label='NLDAS')\n",
    "    constant_all_plt[var].plot(ax=axes[idx],label='Constant')\n",
    "    axes[idx].set_title('') \n",
    "    axes[idx].set_ylabel('{} {}'.format(var, unit_str[idx]))\n",
    "    axes[idx].set_xlabel('Date')\n",
    "plt.tight_layout()\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot cummulative\n",
    "fig, axes = plt.subplots(nrows=7, ncols=1, figsize=(20, 20))\n",
    "axes = axes.flatten()\n",
    "axes[0].set_title('Cumulative')\n",
    "\n",
    "truth_plt = truth.isel(hru=0, time=slice(start+90*24, stop+90*24)).cumsum(dim='time')\n",
    "constant_all_plt = constant_all.isel(hru=0, time=slice(start+90*24, stop+90*24)).cumsum(dim='time')\n",
    "\n",
    "for idx, var in enumerate(variables[0:7]):\n",
    "    truth_plt[var].plot(ax=axes[idx],label='NLDAS')\n",
    "    constant_all_plt[var].plot(ax=axes[idx],label='Constant')\n",
    "    axes[idx].set_title('') \n",
    "    axes[idx].set_ylabel('{} {}'.format(var, unit_str[idx]))\n",
    "    axes[idx].set_xlabel('Date')\n",
    "plt.tight_layout()\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pysumma",
   "language": "python",
   "name": "pysumma"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
