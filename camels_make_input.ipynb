{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating forcing datasets for pySUMMA\n",
    "\n",
    "Here we will put together forcing data setups for CAMELs.\n",
    "This requires hourly forcings of\n",
    " - temperature (K)\n",
    " - precipitation (kg/m^2/s^1)\n",
    " - shortwave radiation (W/m^2)\n",
    " - longwave radiation (W/m^2)\n",
    " - specific humidity (g/g)\n",
    " - air pressure (Pa)\n",
    " - wind speed (m/s)\n",
    " \n",
    "NLDAS hourly forcings are considered to be \"truth\". \n",
    "We take NLDAS hourly forcings and give them their mean daily values for each variable in turn (or all variables). \n",
    "Then we take NLDAS daily forcings of maximum and minimum temperature, total precipitation, and mean windspeed; and redistribute them to hourly with calculation of the other variables using the MetSim python package. \n",
    "Lastly, take NLDAS forcings (hourly) and give them their MetSim valuesv or each variable in turn (or all variables). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start, the first thing we need to do is make sure SUMMA and pysumma are installed. Commands:\n",
    "* `cd /path/to/home`\n",
    "* `github clone https://github.com/ashleymedin/summa.git`\n",
    "* `cd summa`\n",
    "* `github checkout develop`\n",
    "* `cd build`\n",
    "\n",
    "In the makefile, at a minimum you will need to set the following:\n",
    "* F_MASTER         - top level summa directory\n",
    "* FC               - compiler suite\n",
    "* FC_EXE           - compiler executable\n",
    "* INCLUDES         - path to include files\n",
    "* LIBRARIES        - path to and libraries to include\n",
    "\n",
    "This is setup in the repo for Cheyenne under Makefile_cheyenne, which you can copy and then build. Commands:\n",
    "* `cp Makefile_cheyenne my_makefile`\n",
    "* `./build_summa`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, get an Anaconda Python environment set up and install the environment there as follows. \n",
    "\n",
    "Check the operating system:\n",
    "* `cd /path/to/home`\n",
    "* `uname -m`\n",
    "Then get Miniconda for that system. For Linux 64 like Cheyenne, the command is:\n",
    "* `wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh`\n",
    "* `chmod 777 Miniconda3-latest-Linux-x86_64.sh`\n",
    "* `./Miniconda3-latest-Linux-x86_64.sh`\n",
    "Close and then re-open your terminal window.\n",
    "\n",
    "Now install the environment with conda the extra packages, with commands:\n",
    "* `cd /path/to/home`\n",
    "* `git clone https://github.com/ashleymedin/pysumma.git`\n",
    "* `cd pysumma`\n",
    "* `git checkout develop`\n",
    "* `conda env create -f environment.yml`\n",
    "\n",
    "This will make a python environment that you can activate and deactivate. \n",
    "Activate it-- this will install metsim, some plotting tools, and the dask tools for good measure: \n",
    "* `conda activate pysumma` \n",
    "\n",
    "Install it as a kernel in your Jupyter environments:\n",
    "* `python -m ipykernel install --user --name=pysumma`\n",
    "\n",
    "Then install pySUMMA\n",
    "* `python setup.py develop`\n",
    "\n",
    "Make sure to run `git pull origin`inside `/summa` and `/pysumma` periodically to keep things up to date. You need to rebuild SUMMA if there are changes. You *do not* need to run `setup.py` again if you used develop. \n",
    "If you want to deactivate, run `conda deactivate`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "Now, let's make the inputs for pySumma. First we check that we loaded correct environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conda list metsim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "Then we load some standard imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import geoviews as gv\n",
    "import geopandas as gpd\n",
    "import holoviews as hv\n",
    "import xarray as xr\n",
    "import ogr\n",
    "import cartopy\n",
    "from metsim import MetSim\n",
    "from dask_jobqueue import PBSCluster\n",
    "from dask.distributed import Client\n",
    "\n",
    "hv.notebook_extension('bokeh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run MetSim on a scheduler to make sure we have enough memory\n",
    "# Rough guesstimate is you need (8 * chunksize * 10 * nTimesteps / 1e9) GB of memory per worker. \n",
    "# For 40 years of hourly output with 14 HRU/chunk that's about 0.5 GB / worker, so you might need something around 24 GB with 48 workers,\n",
    "NCORES=48\n",
    "cluster = PBSCluster(n_workers = NCORES,\n",
    "                     cores=NCORES,\n",
    "                     processes=NCORES, \n",
    "                     memory=\"48GB\",\n",
    "                     project='UWAS0091',\n",
    "                     queue='regular',\n",
    "                     walltime='02:00:00')\n",
    "client = Client(cluster)\n",
    "client.write_scheduler_file('./scheduler.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check that have workers, do not run the rest of the cells until the workers show up. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(client)\n",
    "!qstat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### You will need to edit these paths to be your folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some folder places\n",
    "top = '/glade/work/ashleyvb'\n",
    "folder = top+'/CAMELs'\n",
    "folders = folder+'/summa_camels'\n",
    "shapefile = folder+'/basin_set_full_res_simple/HCDN_nhru_final_671.shp'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# Our study location: the CAMELs basins\n",
    "\n",
    "We are using MetSim on unstructured mesh arrangements as well as the structured latitude-longitude grid that we ran in the command line examples.\n",
    "We will use a setup which consists of 671 \"hydrologic response units\" or HRU.\n",
    "A HRU typically delineates a watershed by topography, soil type, land use, or other defining feature.\n",
    "Here, each CAMELs basin is run as a lumped model, so each one is an HRU.\n",
    "\n",
    "Let's take a look. \n",
    "Because this is an unstructured mesh we will be defining the mesh elements by their respective basin identifiers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list = []\n",
    "filelist = open(folders+'/settings.v1/forcingFileList.1hr.txt', 'r')\n",
    "for lineNumber, line in enumerate (filelist):\n",
    "   file_list.append(folders+'/forcing/1hr/'+line.strip(\"'\\n\"))\n",
    "filelist.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some things get dumped with open_mfdataset, so keep them here\n",
    "extra_vars0 = xr.open_dataset(file_list[0]) \n",
    "extra_vars0 = extra_vars0.assign_coords(hru=extra_vars0['hruId'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### THIS IS IMPORTANT! Select here what basins you want to run. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All HRUs, select out all or some to look at, assuming have GRUs same as HRUs here\n",
    "#the_hru = np.array(extra_vars0['hruId']) # run all\n",
    "the_hru = np.array([1144000,6353000,9223000,11528700]) # or select some, comment out to run all \n",
    "the_gru = the_hru\n",
    "extra_vars = extra_vars0.sel(hru=the_hru)\n",
    "print(the_hru)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf = gpd.read_file(shapefile)\n",
    "shapes = cartopy.io.shapereader.Reader(shapefile)\n",
    "list(shapes.records())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the data from an xarray dataset to a pandas dataframe \n",
    "out_df = extra_vars0['hru']\n",
    "out_df = out_df.to_dataframe()\n",
    "# Make sure we have some metadata to join with the shapefile\n",
    "out_df['hru_id'] = gdf['hru_id'].values\n",
    "#search for the ones with desired records\n",
    "find_rec = out_df.loc[the_hru,:]['hru_id']\n",
    "#look at attributes\n",
    "desired_shapes = []\n",
    "for i in find_rec:\n",
    "    for s in shapes.records():\n",
    "        if s.attributes['hru_id'] == i :\n",
    "            desired_shapes.append(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create backgound\n",
    "mapp =gv.tile_sources.StamenTerrainRetina.opts(width=900, height=500)\n",
    "# Create the shape plot\n",
    "poly = gv.Shape.from_records(shapes.records(), out_df, index=['hru_id'], on='hru_id',crs=cartopy.crs.PlateCarree())\n",
    "poly2 = gv.Shape.from_records(desired_shapes,out_df.loc[the_hru,:],on='hru_id', crs=cartopy.crs.PlateCarree())\n",
    "poly = poly.opts(cmap='plasma', tools=['hover'], colorbar=True, alpha=0.8)\n",
    "poly2 = poly2.opts(fill_color='cyan', line_color='cyan', alpha=0.8)\n",
    "# Plot\n",
    "mapp*poly*poly2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basins are colored by index values and are hoverable for index values and IDs. Selected basins are higlighted in cyan."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# Make SUMMA files the correct HRUs\n",
    "We have to select out only the HRUs of the basins we are using."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# There was a misname in this file, so write version 2\n",
    "attrib = xr.open_dataset(folders+'/settings.v1/attributes.camels.v1.nc')\n",
    "print(attrib['latitude'])\\n\",\n",
    "attrib = attrib.rename({'longitude':'latitude','latitude':'longitude'})\n",
    "print(attrib['latitude'])\n",
    "attrib.to_netcdf(folders+'/settings.v1/attributes.camels.v2.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attributes \n",
    "attrib = xr.open_dataset(folders+'/settings.v1/attributes.camels.v2.nc')\n",
    "attrib = attrib.assign_coords(hru=attrib['hruId'])\n",
    "attrib = attrib.assign_coords(gru=attrib['gruId'])\n",
    "gg = attrib['gruId'] # save because gruId was missing from the parameter file\n",
    "attrib = attrib.sel(hru=the_hru)\n",
    "attrib = attrib.sel(gru=the_gru)\n",
    "attrib = attrib.drop(['hru','gru']) #summa doesn't like these to have coordinates\n",
    "attrib.to_netcdf(folders+'/settings.v1/attributes.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "param = xr.open_dataset(folders+'/settings.v1/trialParams.camels.v1.nc')\n",
    "param = param.assign_coords(hru=param['hruId'])\n",
    "param = param.assign_coords(gru=gg) # there should be a gruId in here, but there wasn't\n",
    "param = param.sel(hru=the_hru)\n",
    "param = param.sel(gru=the_gru)\n",
    "param = param.drop(['hru','gru']) #summa doesn't like these to have coordinates\n",
    "param.to_netcdf(folders+'/settings.v1/parameters.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "Lastly we will need to make the constant initial conditions file.\n",
    "\n",
    "### You will need to edit these paths to be your folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! cd /glade/work/ashleyvb/CAMELs/summa_camels/settings.v1; source activate /glade/work/ashleyvb/miniconda3/envs/pysumma; python gen_coldstate.py attributes.nc init_cond.nc int"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# Forcing files of NLDAS with Constant Daily Values\n",
    "Here we make the NLDAS data hourly values into constant 24 hourly values for each forcing variable. \n",
    "We need to make these 24 hours represent a local day, so local time zones, such that later calculations on days work. \n",
    "Since SUMMA will impose that shortwave radiation is 0 when the sun is below the horizon, we distribute the constant shortwave radiation only during the daylight hours.\n",
    "Other changes inside SUMMA are that specific humidity will be lowered in order that relative humidity does not exceed 100%, and tiny windspeeds will be elimated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "Merge everything in the folder along time. This takes ~2 minutes.\n",
    "We set 'minimal' so it won't add the time dimension to the variables that don't have it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "truth = xr.open_mfdataset(folders+'/forcing/1hr/*.nc',data_vars='minimal',combine='by_coords').load()\n",
    "truth = truth.assign_coords(hru=truth['hruId'])\n",
    "truth = truth.sel(hru=the_hru)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix time encoding to be the same since the merge drops it\n",
    "truth.time.encoding = extra_vars.time.encoding\n",
    "# Also fix the variables that have no time dimension as they may get merged incorrectly by open_mfdataset\n",
    "truth['data_step'] = extra_vars['data_step']\n",
    "# Pull out HRUs\n",
    "truth = truth.assign_coords(hru=truth['hruId'])\n",
    "truth = truth.sel(hru=the_hru) # none of these in in here, so maybe a dummy file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "Write this file for pySUMMA forcing and save the forcing file name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save\n",
    "t0 = truth['time'].values[0] \n",
    "tl = truth['time'].values[-1]\n",
    "t0_s = pd.to_datetime(str(t0))\n",
    "t0_sf =t0_s.strftime('%Y%m%d')\n",
    "tl_s = pd.to_datetime(str(tl))\n",
    "tl_sf =tl_s.strftime('%Y%m%d')\n",
    "ffname ='NLDAStruth_' + t0_sf +'-' + tl_sf +'.nc'\n",
    "truth.to_netcdf(folders+'/forcing/truth/'+ffname)\n",
    "fflistname = folders+'/settings.v1/forcingFileList.truth.txt' \n",
    "file =open(fflistname,\"w\")\n",
    "file.write(ffname)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "Now we can make the daily data, first shifting to local time zones with longitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate out variables with no time dimension and add offset\n",
    "ds = truth\n",
    "ds_withtime = ds.drop_vars([ var for var in ds.variables if not 'time' in ds[var].dims ])\n",
    "ds_timeless = ds.drop_vars([ var for var in ds.variables if     'time' in ds[var].dims ])\n",
    "DEG_PER_REV = 360.0       # Number of degrees in full revolution\n",
    "HRS_PER_DAY = 24\n",
    "offset = (attrib['longitude'] / DEG_PER_REV) * HRS_PER_DAY\n",
    "offset = offset.astype(int)\n",
    "ds_withtime['offset'] = offset\n",
    "ds_withtime = ds_withtime.assign_coords(hru=ds_timeless['hruId'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "Here are the time zone changes. \n",
    "This takes about a minute using all 671 basins; a subset of basins should be shorter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for t in np.unique(offset.data):\n",
    "    ds = ds_withtime.where(offset==t,drop=True)\n",
    "    ds = ds.shift(time=t)\n",
    "    if t==np.unique(offset.data)[0]: ds0 = ds\n",
    "    if t>np.unique(offset.data)[0]: ds0 = xr.concat([ds0,ds],dim='hru')\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort back, and drop offset, keep airtemp for later calculations\n",
    "ds_withtime = ds0.sortby('hru')\n",
    "ds_withtime = ds_withtime.drop_vars('offset') \n",
    "air24 = ds_withtime.get('airtemp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "Downsample hourly time-series data to daily data. \n",
    "This takes about a 30 seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "truth24 = xr.merge([ds_timeless, ds_withtime.resample(time='1D').mean()]).load()\n",
    "# Fix time encoding to be the same since the merge drops it\n",
    "truth24.time.encoding = extra_vars.time.encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "Then we upsample this back to hourly data for constant daily values. \n",
    "We need to undo the time zone changes after we upsample. \n",
    "This whole process takes about a minute using all 671 basins; a subset of basins should be shorter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a fake day of data so upsamples until the end\n",
    "day_fake = truth24.isel(time=-1)['time']+np.timedelta64(1,'D')\n",
    "add_fake = truth24.isel(time=-1)\n",
    "add_fake['time'] = day_fake\n",
    "truth24_add = xr.concat([truth24, add_fake], dim='time',data_vars='minimal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Again we have to separate out variables with no time dimension.\n",
    "ds = truth24_add\n",
    "ds_withtime = ds.drop_vars([ var for var in ds.variables if not 'time' in ds[var].dims ])\n",
    "ds_timeless = ds.drop_vars([ var for var in ds.variables if     'time' in ds[var].dims ])\n",
    "ds_withtime = ds_withtime.resample(time='1H').ffill()\n",
    "ds_withtime['offset'] = offset\n",
    "ds_withtime = ds_withtime.assign_coords(hru=ds_timeless['hruId'])\n",
    "for t in np.unique(offset.data):\n",
    "    ds = ds_withtime.where(offset==t,drop=True)\n",
    "    ds = ds.shift(time=-t)\n",
    "    if t==np.unique(offset.data)[0]: ds0 = ds\n",
    "    if t>np.unique(offset.data)[0]: ds0 = xr.concat([ds0,ds],dim='hru')\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort back, and drop offset, and merge\n",
    "ds_withtime = ds0.sortby('hru')\n",
    "ds_withtime = ds_withtime.drop_vars('offset') \n",
    "constant_all = xr.merge([ds_timeless, ds_withtime])\n",
    "constant_all = constant_all.sel(hru=the_hru) #put back in original order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take extra day off\n",
    "constant_all = constant_all.isel(time=slice(0,-1))\n",
    "# Fix time encoding to be the same since the merge drops it\n",
    "constant_all.time.encoding = extra_vars.time.encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Scale Constant SW Radiation\n",
    "Edit the constant daily shortwave radiation so that energy is the same in the \"truth\" when pySUMMA makes the shortwave radiation zero during the day.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find where 0's shoud be based on original NLDAS data\n",
    "zero_one = truth['SWRadAtm']/truth['SWRadAtm']\n",
    "zero_one = zero_one.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find how much too small shortwave is each day if we use these 0's\n",
    "swr0 = zero_one*constant_all['SWRadAtm']\n",
    "div = swr0.resample(time='1D').mean()/constant_all['SWRadAtm'].resample(time='1D').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upsample, again add a fake day of data so upsamples until the end\n",
    "add_fake = div.isel(time=-1)\n",
    "add_fake['time'] = day_fake\n",
    "div_add = xr.concat([div, add_fake], dim='time')\n",
    "div_add = div_add.resample(time='1H').ffill()\n",
    "\n",
    "# Take extra day off\n",
    "div = div_add.isel(time=slice(0,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally add back in this constant shortwave radiation\n",
    "swr0 = swr0/div\n",
    "constant_all['SWRadAtm']=swr0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "Finally, write this file for pySUMMA forcing and save the forcing file name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = constant_all['time'].values[0] \n",
    "tl = constant_all['time'].values[-1]\n",
    "t0_s = pd.to_datetime(str(t0))\n",
    "t0_sf =t0_s.strftime('%Y%m%d')\n",
    "tl_s = pd.to_datetime(str(tl))\n",
    "tl_sf =tl_s.strftime('%Y%m%d')\n",
    "ffname ='NLDASconstant_all_forcing_' + t0_sf +'-' + tl_sf +'.nc'\n",
    "constant_all.to_netcdf(folders+'/forcing/constant/'+ffname)\n",
    "fflistname = folders+'/settings.v1/forcingFileList.constant_all.txt' \n",
    "file =open(fflistname,\"w\")\n",
    "file.write(ffname)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Files with Only One Variable Constant\n",
    "Now make files with only one variable held at daily means and save forcing file names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "constant_vars=['airpres','airtemp','LWRadAtm','pptrate','spechum','SWRadAtm','windspd']\n",
    "for v in constant_vars:\n",
    "    constant_one = truth.copy()\n",
    "    constant_one[v]= constant_all[v]\n",
    "    ffname ='NLDASconstant_' + v +'_forcing_' + t0_sf +'-' + tl_sf +'.nc'\n",
    "    constant_one.to_netcdf(folders+'/forcing/constant/'+ffname)\n",
    "    fflistname = folders+'/settings.v1/forcingFileList.constant_' + v + '.txt' \n",
    "    file =open(fflistname,\"w\")\n",
    "    file.write(ffname)\n",
    "    file.close()\n",
    "    print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# MetSim Input Setup\n",
    "\n",
    "Now we can use the daily NLDAS file to setup MetSim. We need to separate the raw data into a domain file (the attributes of the domain), a state file (the data for the first 90 days), and a forcing file (the data after 90 days). Here's the starting data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate daily max and min temps, get precip, and wind speed, and fix units\n",
    "truth24_met = truth24.get(['pptrate','windspd'])\n",
    "truth24_met['maxtemp'] = air24.resample(time='1D').max()-273.15 # units C from K\n",
    "truth24_met['mintemp'] = air24.resample(time='1D').min()-273.15 # units C from K\n",
    "truth24_met['pptrate'] = truth24_met['pptrate']*3600*24 # units mm/day from mm/s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "Next we need to make the domain file from the attributes. Also in the domain file, we need to add the parameters for the Precipitation Isosceles Triangle (PITRI) precipitation disaggregation method (Bohn et al., 2019), which have been calculated for a lat long grid here https://zenodo.org/record/2564019#.XorEBW5S_xU. We load this file and identify the parameters closest to each HRU lat long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain = attrib.get(['latitude','longitude','elevation'])\n",
    "# Add the mask to domain\n",
    "shape = (len(truth24_met['hru']), )\n",
    "coords = {'hru': truth24_met['hru'] }\n",
    "domain['mask'] = xr.DataArray(data=np.full(shape,1),\n",
    "                          coords=coords,dims=('hru',),\n",
    "                          name='mask')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add t_pk and dur from the Bohn grid\n",
    "par_precip = xr.open_dataset(folder+'/MetSim/input/domain.CONUS_MX.L2015.nc')\n",
    "# Load so the select only operates one one HRU at a time instead of HRU*HRU\n",
    "t_pk = par_precip['t_pk'].load()\n",
    "# Identifing by nearest neighbor\n",
    "# reset coords to get rid of lat and long as coords and on.isel(time=slice(0))ly select t_pk, dur as variables\n",
    "t_pk_sel = t_pk.sel(lat=domain['latitude'],lon=domain['longitude'],method='nearest').reset_coords()['t_pk'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dur = par_precip['dur'].load() #\n",
    "dur_sel = dur.sel(lat=domain['latitude'],lon=domain['longitude'],method='nearest').reset_coords()['dur']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain['t_pk'] = t_pk_sel \n",
    "domain['dur'] = dur_sel \n",
    "domain.to_netcdf(folder+'/MetSim/input/DAILYnldas_domain.nc')\n",
    "print(domain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "Lastly, separate out the state and forcing files in the met data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the state file\n",
    "state = truth24_met.isel(time=slice(0,90))\n",
    "state.to_netcdf(folder+'/MetSim/input/DAILYnldas_state.nc')\n",
    "print(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print these if things are going wrong\n",
    "#print(state.time.encoding)\n",
    "#print(xr.open_dataset(folder+'/MetSim/input/DAILYnldas_state.nc', decode_times=False).time[0])\n",
    "#print(xr.open_dataset(folder+'/MetSim/input/DAILYnldasINIT.nc', decode_times=False).time[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the forcing file for MetSim\n",
    "forcing = truth24_met.isel(time=slice(90,None)) #should be slice(90,None)\n",
    "forcing.to_netcdf(folder+'/MetSim/input/DAILYnldas_forcing.nc')\n",
    "print(forcing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# MetSim Parameters and Instantiation\n",
    "\n",
    "While in the scripting enviornment we will provide the configuration as a Python dictionary. \n",
    "The information that this dictionary, which we will call `params`, is similar to that which is found in the configuration file.\n",
    "However, there are some notable differences which we will walk through.\n",
    "\n",
    "First, note that we must provide dates as `datetime` objects that is the time of the forcing file \n",
    "This is easily handled via `pandas`. \n",
    "Most variables are self-explanatory, so we only define\n",
    "* `time_step` - this is the time step of the output (in minutes). There are some limiataions on what this is allowed to be. Of course, it can't be greater than 1440 (number of minutes in a day). But it also must be less than 360 (or 6 hours) for numerical reasons. We also should note that, while you can go down to output on the minute, it really doesn't make sense to set this lower than 30.\n",
    "* `scheduler` - the scheduler is how MetSim manages jobs under the hood. We can now use the `distributed` scheduler in this sort of interactive environment and it will be much faster than `threading`. We've already set this up above to have enough memory.\n",
    "* `chunks` - this specifies how the domain will be broken up spatially to be run as a separate job. We set it to {'hru': 14} to have 14 HRUs run at a time\n",
    "* `numworkers` - this multiplied by `chunks` should be around the number of hrus; here 671. We can't have more than 48 workers, but we need to have a lot so each worker does not exceed it's memory.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = forcing['time'].values[0] \n",
    "tl = forcing['time'].values[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rest of the workflow\n",
    "dates = pd.date_range(t0,tl)\n",
    "params = {\n",
    "    # Run parameters, timestep, start, and stop dates\n",
    "    'time_step'    : \"60\",       \n",
    "    'start'        : dates[0],\n",
    "    'stop'         : dates[-1],\n",
    "    # Where all of our input files are\n",
    "    'forcing'      : folder+'/MetSim/input/DAILYnldas_forcing.nc',     \n",
    "    'domain'       : folder+'/MetSim/input/DAILYnldas_domain.nc',\n",
    "    'state'        : folder+'/MetSim/input/DAILYnldas_state.nc',\n",
    "    # The format of the forcing data\n",
    "    'forcing_fmt'  : 'netcdf',\n",
    "    # Distribution of precipitation. Under mix, the “uniform” method is used on days when t_min < 0 C, and “triangle” is used on all\n",
    "    # other days; this hybrid method retains the improved accuracy of “triangle” in terms of warm season runoff but avoids the biases in\n",
    "    # snow accumulation that the “triangle” method sometimes yields due to fixed event timing within the diurnal cycle of temperature. \n",
    "    'prec_type'  : 'mix',\n",
    "    # Whether to use UTC timecode offsets for shifting timeseries. Without this option all times should be considered local \n",
    "    #to the gridcell being processed. Large domain runs probably want to set this option to True.\n",
    "    'utc_offset'  : 'True',\n",
    "    # tmax_daylength_fraction :: float : Weight for calculation of time of maximum daily temperature. Must be between 0 and 1. Defaults to 0.67. \n",
    "    'tmax_daylength_fraction': 0.67,\n",
    "    # Specification of the output\n",
    "    'out_dir'      : folder+'/MetSim/output',\n",
    "    'output_prefix': 'DAILYnldas',\n",
    "    # This does not work, it keeps the output at default names\n",
    "    'out_vars'     :\n",
    "         {'temp':'airtemp', \n",
    "          'prec':'pptrate', \n",
    "          'shortwave':'SWRadAtm', \n",
    "          'longwave':'LWRadAtm', \n",
    "          'spec_humid':'spechum', \n",
    "          'air_pressure':'airpres', \n",
    "          'wind':'windspd'},\n",
    "    # also available 'tsck' (cloud cover frac), 'rel_humid','vapor_pressure'\n",
    "    # Scheduler to use for job management - see text above for more information\n",
    "    'scheduler'      :'./scheduler.json',\n",
    "    # This is analagous to the [chunks] section from the config file\n",
    "    'num_workers'    : 24,\n",
    "    'chunks'       : \n",
    "        {'hru': 7},\n",
    "    # This is analagous to the [forcing_vars] section from the config file\n",
    "    'forcing_vars' : \n",
    "         {'pptrate': 'prec', \n",
    "          'maxtemp': 't_max', \n",
    "          'mintemp': 't_min', \n",
    "          'windspd': 'wind'},\n",
    "    # This is analagous to the [state_vars] section from the config file\n",
    "    'state_vars'   : \n",
    "         {'pptrate': 'prec', \n",
    "          'maxtemp': 't_max', \n",
    "          'mintemp': 't_min', \n",
    "          'windspd': 'wind'},\n",
    "    # This is analagous to the [domain_vars] section from the config file\n",
    "    'domain_vars'  : \n",
    "        {'elevation': 'elev', \n",
    "         'latitude': 'lat', \n",
    "         'longitude': 'lon', 'mask': 'mask',\n",
    "          'dur':'dur',\n",
    "          't_pk':'t_pk'}\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "Now create the object and run the simulation. This takes about 3 minutes using all 671 basins and shorter with fewer basins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ms = MetSim(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ms.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# Modify MetSim Output to pySUMMA Input\n",
    "\n",
    "Let's look at the MetSim output. It's got the 671 HRU each with 339720 timesteps (equal to 14155 * 24).\n",
    "We have 7 variables:\n",
    " - temperature (C)\n",
    " - precipitation (mm/hr)\n",
    " - shortwave radiation (W/m^2)\n",
    " - longwave radiation (W/m^2)\n",
    " - specific humidity (g/g)\n",
    " - air pressure (kPa)\n",
    " - wind speed (m/s)\n",
    " \n",
    "Other options that we do not need for pySumma but we could make:\n",
    " - vapor pressure (kPa)\n",
    " - relative humidity (%)\n",
    " - cloud cover fraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0_s = pd.to_datetime(str(t0))\n",
    "t0_sf =t0_s.strftime('%Y%m%d')\n",
    "tl_s = pd.to_datetime(str(tl))\n",
    "tl_sf =tl_s.strftime('%Y%m%d')\n",
    "# With distributed we need to read the file instead of just the ms.open_output()\n",
    "ms_out = xr.open_dataset(folder+'/MetSim/output/forcing_' + t0_sf +'-' + tl_sf +'.nc') \n",
    "ms_out = ms_out.sel(hru=the_hru) #put back in original order\n",
    "print(ms_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "To use this for forcing in pySUMMA, we will need to rename some variables, change the units, add a few variables, and fix the time encoding before we write the file. \n",
    "This first step takes about a minute using all 671 basins; a subset of basins should be shorter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Rename variables\n",
    "ms_pysum = ms_out.rename({\n",
    "          'temp':'airtemp', \n",
    "          'prec':'pptrate', \n",
    "          'shortwave':'SWRadAtm', \n",
    "          'longwave':'LWRadAtm', \n",
    "          'spec_humid':'spechum', \n",
    "          'air_pressure':'airpres', \n",
    "          'wind':'windspd'}).load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change units\n",
    "ms_pysum['airtemp'] = ms_pysum['airtemp']+273.15 # units K from C\n",
    "ms_pysum['pptrate'] = ms_pysum['pptrate']/3600 # mm/s, unit kg m-2 s-1, from mm/h\n",
    "ms_pysum['airpres'] = ms_pysum['airpres']*1000 # units Pa from KPa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add in SUMMA variables from original files, or we could write these ourselves\n",
    "ms_pysum['data_step'] = extra_vars['data_step']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix time encoding to be the same as truth\n",
    "ms_pysum.time.encoding = extra_vars.time.encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "Finally, write this file for pySUMMA forcing and save the forcing file name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add it into truth instead of just writing new file so that all forcing files start at the same time.\n",
    "metsim_vars=['airpres','airtemp','LWRadAtm','pptrate','spechum','SWRadAtm','windspd']\n",
    "metsim_one = truth.copy()\n",
    "for v in metsim_vars:\n",
    "    metsim_one[v]= ms_pysum[v]\n",
    "ffname ='NLDASmetsim_' + 'all' +'_forcing_' + t0_sf +'-' + tl_sf +'.nc'\n",
    "metsim_one.to_netcdf(folders+'/forcing/metsim/'+ffname)\n",
    "fflistname = folders+'/settings.v1/forcingFileList.metsim_all.txt' \n",
    "file =open(fflistname,\"w\")\n",
    "file.write(ffname)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Files with Only One Variable Metsim\n",
    "Now make files with only one variable held at MetSim and save forcing file names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for v in metsim_vars:\n",
    "    metsim_one = truth.copy()\n",
    "    metsim_one[v]= ms_pysum[v]\n",
    "    ffname ='NLDASmetsim_' + v +'_forcing_' + t0_sf +'-' + tl_sf +'.nc'\n",
    "    metsim_one.to_netcdf(folders+'/forcing/metsim/'+ffname)\n",
    "    fflistname = folders+'/settings.v1/forcingFileList.metsim_' + v +'.txt' \n",
    "    file =open(fflistname,\"w\")\n",
    "    file.write(ffname)\n",
    "    file.close()\n",
    "    print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# Check Files\n",
    "\n",
    "To make sure things look how we want, we plot the MetSim dataset against the NLDAS \"truth\" dataset and the constant dataset, and plot the cumulative variables to see how errors are compounding. \n",
    "We plot one HRU (the first one) for 2 months."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot hourly\n",
    "fig, axes = plt.subplots(nrows=7, ncols=1, figsize=(20, 20))\n",
    "axes = axes.flatten()\n",
    "axes[0].set_title('Hourly')\n",
    "\n",
    "unit_str = ['($ ^o K$)', '($kg/m/s$)', '($W/m^2$)','($w/m^2$)','($g/g$)','($Pa$)', '($m/s$)',]\n",
    "\n",
    "variables = list(ms_pysum.variables.keys())\n",
    "dims = list(ms_out.dims.keys())\n",
    "[variables.remove(d) for d in dims]\n",
    "\n",
    "start =  24*7*30 \n",
    "stop = start + 2*30*24 \n",
    "#truth starts 90 days earlier\n",
    "truth_plt = truth.isel(hru=0, time=slice(start+90*24, stop+90*24))\n",
    "constant_all_plt = constant_all.isel(hru=0, time=slice(start+90*24, stop+90*24))\n",
    "ms_pysum_plt = ms_pysum.isel(hru=0, time=slice(start, stop))\n",
    "\n",
    "for idx, var in enumerate(variables[0:7]):\n",
    "    truth_plt[var].plot(ax=axes[idx],label='NLDAS')\n",
    "    constant_all_plt[var].plot(ax=axes[idx],label='Constant')\n",
    "    ms_pysum_plt[var].plot(ax=axes[idx],label='MetSim')\n",
    "    axes[idx].set_title('') \n",
    "    axes[idx].set_ylabel('{} {}'.format(var, unit_str[idx]))\n",
    "    axes[idx].set_xlabel('Date')\n",
    "plt.tight_layout()\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot cummulative\n",
    "fig, axes = plt.subplots(nrows=7, ncols=1, figsize=(20, 20))\n",
    "axes = axes.flatten()\n",
    "axes[0].set_title('Cumulative')\n",
    "\n",
    "truth_plt = truth.isel(hru=0, time=slice(start+90*24, stop+90*24)).cumsum(dim='time')\n",
    "constant_all_plt = constant_all.isel(hru=0, time=slice(start+90*24, stop+90*24)).cumsum(dim='time')\n",
    "ms_pysum_plt = ms_pysum.isel(hru=0, time=slice(start, stop)).cumsum(dim='time')\n",
    "\n",
    "for idx, var in enumerate(variables[0:7]):\n",
    "    truth_plt[var].plot(ax=axes[idx],label='NLDAS')\n",
    "    constant_all_plt[var].plot(ax=axes[idx],label='Constant')\n",
    "    ms_pysum_plt[var].plot(ax=axes[idx],label='MetSim')\n",
    "    axes[idx].set_title('') \n",
    "    axes[idx].set_ylabel('{} {}'.format(var, unit_str[idx]))\n",
    "    axes[idx].set_xlabel('Date')\n",
    "plt.tight_layout()\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pysumma",
   "language": "python",
   "name": "pysumma"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
