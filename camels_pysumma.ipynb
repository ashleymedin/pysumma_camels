{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running the SUMMA setups\n",
    "To begin, we have to regionalize the paths in the configuration files that SUMMA will use.\n",
    "This is accomplished by running a shell command. This is done by starting a line with the `!` operator.\n",
    "We simply run a script to complete the installation.\n",
    "Then, we can import some basic libraries along with `pysumma`.\n",
    "Note, if you run this with all 671 basins, you may be maxing out memory. \n",
    "If things start to fail, restart the server and run the simuluations in smaller loops before restarts. Also, make sure you start with 16GB per node on the Cheyenne PBS Job Options. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### You will need to edit these paths to be your folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top = '/glade/work/ashleyvb'\n",
    "folder = top+'/CAMELs'\n",
    "folders = folder+'/summa_camels'\n",
    "! cd /glade/work/ashleyvb/CAMELs/summa_camels; ./install_local_setup.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we check that we loaded the correct environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conda list pysumma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "Then we load the imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pysumma as ps\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "from dask_jobqueue import PBSCluster\n",
    "from dask.distributed import Client\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Make problem complexity choices here:\n",
    "\n",
    "Now decide if you want to run: \n",
    " - `default_prob = 1`: the \"default\" configuration with the \"default\" parameters. By \"default\" we mean whatever you chose in the summa setup files. \n",
    " - `lhc_prob = 1`: the default configuration with exploration of the parameter space.\n",
    " - `lhc_config_prob = 1`: 8 different configurations (choices that have been seen to affect the model output in previous research) with exploration of the parameter space.\n",
    " \n",
    "Note, you can choose all of these to be 1, but each step of complexity will contain the previous problem(s), so it is not necessary. It will result in more files, but if may be useful to check that each step of the problem expansion runs successfullly. If you have more than 10 HRUs (CAMELs subbasins in the example problem), it was decided that the problem is too big and you can only run the default problem. However, theoretically it will run with more HRUs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get number of HRUs\n",
    "attrib = xr.open_dataset(folders+'/settings.v1/attributes.nc')\n",
    "the_hru = np.array(attrib['hruId'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_prob = 0\n",
    "lhc_prob = 0\n",
    "lhc_config_prob = 1\n",
    "if len(the_hru) >10:\n",
    "    lhc_prob = 0\n",
    "    lhc_config_prob = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# smaller problem needs less power, if your problem is particularly small you might be able to get by with even less and not make the kernel die.\n",
    "# You can also add more time, up to 06:00:00\n",
    "if len(the_hru) >10:\n",
    "    NCORES=48\n",
    "    cluster = PBSCluster(n_workers = NCORES,\n",
    "                     cores=NCORES,\n",
    "                     processes=NCORES, \n",
    "                     memory=\"24GB\",\n",
    "                     project='UWAS0091',\n",
    "                     queue='share',\n",
    "                     walltime='03:00:00')\n",
    "else:\n",
    "    NCORES=32\n",
    "    cluster = PBSCluster(n_workers = NCORES,\n",
    "                         cores=NCORES,\n",
    "                         processes=NCORES, \n",
    "                         memory=\"16GB\",\n",
    "                         project='UWAS0091',\n",
    "                         queue='share',\n",
    "                         walltime='03:00:00')\n",
    "\n",
    "client = Client(cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check that have workers, do not run the rest of the cells until the workers show up. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(client)\n",
    "!qstat\n",
    "# put Job id in here and run if workers are not showing up\n",
    "# !qdel $Job id$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# Interacting with SUMMA via the `Distributed` object\n",
    "\n",
    "We are running a `Distributed` object, which has multiple `Simulation` objects inside, each corresponding to some spatial chunk. \n",
    "We need to do `rm -r /glade/work/ashleyvb/CAMELs/summa_camels/.pysumma/` to clear out the distributed folders every run so permissions do not get screwed up in the loops. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for fewer basins, do not exceed number of basins in chunking\n",
    "CHUNK = 8 #for all 671 basins\n",
    "if len(the_hru) <8: CHUNK = len(the_hru)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "To set up a `Distributed` object you must supply several pieces of information. \n",
    "First, supply the SUMMA executable; this could be either the compiled executable on your local machine, or a docker image. \n",
    "The second piece of information is the path to the file manager, which we just created through the install script. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "executable = top+'/summa/bin/summa.exe'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_manager = folders+'/file_manager_truth.txt'\n",
    "camels = ps.Distributed(executable, file_manager, num_workers=NCORES, chunk_size=CHUNK, client=client)\n",
    "print(camels.manager) #possible days 1980-01-01 to 2018-12-31, we are running 1986-10-01 01:00 to 1991-10-02 0:00"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# pySUMMA with all Forcing Files\n",
    "\n",
    "We run pySumma for each set of forcing files on each basin. You can check how long it has been running by using the command `qstat -u <username>` in a terminal. Each run takes about 9 minutes for 671 basins (shorter if a subset). First, we start with the original NLDAS files, or the \"truth run\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# truth, default parameters and configuration\n",
    "if default_prob==1:\n",
    "    camels.run('local')\n",
    "    all_status = [(n, s.status) for n, s in camels.simulations.items()] #if want to look at status if has errors\n",
    "    all_ds = [s.output.load() for n, s in camels.simulations.items()] #load it into memory so faster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "We could just write it as several files instead of merging. However, if we want to merge, we can do the following.\n",
    "First, detect automatically which vars have hru vs gru dimensions (depending on what we use for output, we may not have any gru):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if default_prob==1:\n",
    "    hru_vars = [] # variables that have hru dimension\n",
    "    gru_vars = [] # variables that have gru dimension\n",
    "    for ds in all_ds:\n",
    "        for name, var in ds.variables.items():\n",
    "            if 'hru' in var.dims:\n",
    "                hru_vars.append(name)\n",
    "            elif 'gru' in var.dims:\n",
    "                gru_vars.append(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "Filter variables for merge, this takes seconds since we are running a limiited output, but if you add more to the output it will take longer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if default_prob==1:\n",
    "    hru_ds = [ds[hru_vars] for ds in all_ds]\n",
    "    gru_ds = [ds[gru_vars] for ds in all_ds]\n",
    "    hru_merged = xr.concat(hru_ds, dim='hru')\n",
    "    gru_merged = xr.concat(gru_ds, dim='gru')\n",
    "    print(hru_merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if default_prob==1:\n",
    "    hru_merged.to_netcdf(folders+'/output/merged_day/NLDAStruth_hru.nc')\n",
    "    gru_merged.to_netcdf(folders+'/output/merged_day/NLDAStruth_gru.nc')\n",
    "    del camels\n",
    "    del all_ds \n",
    "    del hru_merged\n",
    "    del gru_merged\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "Here are the other runs with each forcing held constant, now as a loop. This will take about an hour for each loop using all 671 basins, and about a minute for each loop using 4 basins. We delete stuff after every run to reduce memory needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Each forcing constant, default parameters and configuration\n",
    "if default_prob==1:\n",
    "    constant_vars= ['airpres','airtemp','LWRadAtm','pptrate','spechum','SWRadAtm','windspd']\n",
    "    for v in constant_vars:\n",
    "        ! rm -rf /glade/work/ashleyvb/CAMELs/summa_camels/.pysumma \n",
    "        file_manager = folders+'/file_manager_constant_' + v +'.txt'\n",
    "        camels = ps.Distributed(executable, file_manager, num_workers=NCORES, chunk_size=CHUNK, client=client)   \n",
    "        camels.run('local')\n",
    "        all_status = [(n, s.status) for n, s in camels.simulations.items()] #if want to look at status if has errors\n",
    "        all_ds = [s.output.load() for n, s in camels.simulations.items()] #load it into memory so faster    \n",
    "        hru_vars = [] # variables that have hru dimension\n",
    "        gru_vars = [] # variables that have gru dimension\n",
    "        for ds in all_ds:\n",
    "            for name, var in ds.variables.items():\n",
    "                if 'hru' in var.dims:\n",
    "                    hru_vars.append(name)\n",
    "                elif 'gru' in var.dims:\n",
    "                    gru_vars.append(name)\n",
    "        hru_ds = [ds[hru_vars] for ds in all_ds]\n",
    "        gru_ds = [ds[gru_vars] for ds in all_ds]\n",
    "        hru_merged = xr.concat(hru_ds, dim='hru')\n",
    "        gru_merged = xr.concat(gru_ds, dim='gru')\n",
    "        hru_merged.to_netcdf(folders+'/output/merged_day/NLDASconstant_' + v +'_hru.nc')\n",
    "        gru_merged.to_netcdf(folders+'/output/merged_day/NLDASconstant_' + v +'_gru.nc')\n",
    "        del camels\n",
    "        del all_ds \n",
    "        del hru_merged\n",
    "        del gru_merged\n",
    "        gc.collect()\n",
    "        print(v)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Here is the code to run a smaller test set that is not in distributed mode if you have errors and want to see them.\n",
    "s = ps.Simulation(executable, file_manager)\n",
    "s.manager['simStartTime'] = '1980-04-01 01:00'\n",
    "s.manager['simEndTime'] = '1980-04-03 00:00'\n",
    "print(s.decisions)\n",
    "s.run('local', run_suffix='_default')\n",
    "assert s.status == 'Success'"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(s.stderr)\n",
    "print(s.stdout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# Exploring the Parameter Calibration Space with a Latin Hyper cube\n",
    "\n",
    "The above models were run on the default parameter set only. Let's rerun with parameter sets selected by using a Latin Hyper cube to get 10 different parameter sets for every HRU, in order to explore the calibration space. This will show us if the results of forcing importance could change after calibration. \n",
    "\n",
    "Currrently, none of the model parameters (or decisions) can be altered in a `Distributed` object. \n",
    "However, if we switch to `Simulation` objects and use the `Ensemble` class, we can run suites of different model parameters with relative ease. \n",
    "\n",
    "We change only the parameters that are usually calibrated. You can remove parameters if you were not planning to ever calibrate them away from their defaults (likewise you could add parameters).\n",
    "\n",
    "The absolute minimums and maximums will break simulations and zero out variables, so we do not use those, we stay at the 5% level away from the extremes. Also, there are some constraints on the parameters that must be followed, they are:\n",
    "\n",
    "* heightCanopyTop   > heightCanopyBottom\n",
    "* critSoilTranspire > theta_res\n",
    "* theta_sat         > critSoilTranspire\n",
    "* fieldCapacity     > theta_res\n",
    "* theta_sat         > fieldCapacity\n",
    "* theta_sat         > theta_res\n",
    "* critSoilTranspire > critSoilWilting\n",
    "* critSoilWilting   > theta_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "    \n",
    "Since we must have a small subset of basins, we will proceed with the ensemble calculations with the parameter space\n",
    "Afterwards, you can run the notebook `camels_analyze_subset_output.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if lhc_prob==1 or lhc_config_prob==1: from pyDOE import lhs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if lhc_prob==1 or lhc_config_prob==1:\n",
    "    file_manager = folders+'/file_manager_truth.txt'\n",
    "    s = ps.Simulation(executable, file_manager)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the default, min, and max as in /settings.v1/localParamInfo.txt and /settings.v1/basinParamInfo.txt\n",
    "if lhc_prob==1 or lhc_config_prob==1:\n",
    "    param_calib_hru = ['albedoRefresh', 'aquiferBaseflowExp', 'aquiferBaseflowRate', 'frozenPrecipMultip', 'heightCanopyBottom','heightCanopyTop', 'k_macropore', \n",
    "                   'k_soil', 'qSurfScale', 'summerLAI', 'tempCritRain', 'theta_sat', 'windReductionParam'] \n",
    "    param_calib_gru = ['routingGammaScale', 'routingGammaShape']\n",
    "\n",
    "    for k in param_calib_hru:\n",
    "        print(s.global_hru_params[k])\n",
    "    for k in param_calib_gru:\n",
    "        print(s.global_gru_params[k]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if lhc_prob==1 or lhc_config_prob==1:\n",
    "    bounds_hru = np.full((len(param_calib_hru),3),1.0)\n",
    "    bounds_gru = np.full((len(param_calib_gru),3),1.0)\n",
    "    for i,k in enumerate(param_calib_hru): bounds_hru[i,]= s.global_hru_params.get_value(k)[0:3]\n",
    "    for i,k in enumerate(param_calib_gru): bounds_gru[i,]= s.global_gru_params.get_value(k)[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define bounds and expand to size of LHS runs\n",
    "if lhc_prob==1 or lhc_config_prob==1:\n",
    "    numl = 10\n",
    "    num_vars =  len(param_calib_hru) + len(param_calib_gru)\n",
    "    names =  param_calib_hru + param_calib_gru\n",
    "    bounds =  np.concatenate((bounds_hru, bounds_gru), axis=0)\n",
    "    par_def = dict(zip(names, np.transpose(np.tile(bounds[:,0],(len(the_hru),1))) ))\n",
    "    par_min = dict(zip(names, np.transpose(np.tile(bounds[:,1],(numl*len(the_hru),1))) ))\n",
    "    par_max = dict(zip(names, np.transpose(np.tile(bounds[:,2],(numl*len(the_hru),1))) ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure to obey parameter constraints\n",
    "if lhc_prob==1 or lhc_config_prob==1:\n",
    "    param = xr.open_dataset(folders+'/settings.v1/parameters.nc')\n",
    "\n",
    "    for i,h in enumerate(the_hru):\n",
    "        lb_theta_sat = max(param[['critSoilTranspire','fieldCapacity','theta_res']].isel(hru=i).values()).values\n",
    "        for j in range(0,numl): #say first numl belong to hru 0, second numl to hru 1, and so on\n",
    "            if (par_min['theta_sat'][j + i*numl]<lb_theta_sat): par_min['theta_sat'][j + i*numl]=lb_theta_sat\n",
    "\n",
    "    par_min['heightCanopyTop'] = par_max['heightCanopyBottom']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a 5% buffer\n",
    "if lhc_prob==1 or lhc_config_prob==1:\n",
    "    buff = {key: (par_max.get(key) - par_min.get(key))*0.05 for key in set(par_max) }\n",
    "    par_min = {key: par_min.get(key) + buff.get(key)*0.05 for key in set(buff) }\n",
    "    par_max = {key: par_max.get(key) - buff.get(key)*0.05 for key in set(buff) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate samples with Latin Hypercube Sampling\n",
    "if lhc_prob==1 or lhc_config_prob==1:\n",
    "    np.random.seed(1111)\n",
    "    lhd = lhs(numl*len(the_hru), samples=num_vars)\n",
    "    lhd = dict(zip(names,lhd))\n",
    "    samples = {key: par_min.get(key) + lhd.get(key)*(par_max.get(key) - par_min.get(key)) for key in set(par_max) }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "Set 0 will be the default parameter set. This means the the default parameter set will be run again. We do this so that each finished SUMMA *.nc output file is complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make ensembles\n",
    "if lhc_prob==1 or lhc_config_prob==1:\n",
    "    latin = {}\n",
    "    latin[str(0)] = {'trial_parameters': {key: par_def.get(key) for key in set(par_def) }}\n",
    "    for j in range(0,numl):\n",
    "            latin[str(j+1)] = {'trial_parameters': {key: samples.get(key)[np.arange(j, len(the_hru)*numl, numl)] for key in set(samples) }}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Now we just do what we did before in the simulations previously, except here we merge with a new dimension of the the configuration decision identifier instead of by `hru` and `gru`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# truth, parameter space with default configuration\n",
    "if lhc_prob==1:\n",
    "    param_ens = ps.Ensemble(executable, latin, file_manager, num_workers=NCORES, client=client)   \n",
    "    param_ens.run('local')\n",
    "    all_status = [(n, s.status) for n, s in param_ens.simulations.items()] #if want to look at status if has errors\n",
    "    all_ds = [s.output.load() for n, s in param_ens.simulations.items()] #load it into memory so faster  \n",
    "    all_name = [n for n, s in param_ens.simulations.items()]\n",
    "    all_merged = xr.concat(all_ds, pd.Index(all_name, name=\"decision\"))\n",
    "    all_merged.to_netcdf(folders+'/output/merged_day/NLDAStruth_latin.nc')\n",
    "    del param_ens\n",
    "    del all_ds \n",
    "    del all_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that all were a success\n",
    "if lhc_prob==1:\n",
    "    print(all_status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Each forcing constant, parameter space with default configuration\n",
    "if lhc_prob==1:\n",
    "    constant_vars= ['airpres','airtemp','LWRadAtm','pptrate','spechum','SWRadAtm','windspd']\n",
    "    for v in constant_vars:\n",
    "        ! rm -rf /glade/work/ashleyvb/CAMELs/summa_camels/.pysumma \n",
    "        file_manager = folders+'/file_manager_constant_' + v +'.txt'\n",
    "        s = ps.Simulation(executable, file_manager)\n",
    "        s._write_configuration()\n",
    "        param_ens = ps.Ensemble(executable, latin, file_manager, num_workers=NCORES, client=client)   \n",
    "        param_ens.run('local')\n",
    "        all_status = [(n, s.status) for n, s in param_ens.simulations.items()] #if want to look at status if has errors\n",
    "        all_ds = [s.output.load() for n, s in param_ens.simulations.items()] #load it into memory so faster  \n",
    "        all_name = [n for n, s in param_ens.simulations.items()]\n",
    "        all_merged = xr.concat(all_ds, pd.Index(all_name, name=\"decision\"))\n",
    "        all_merged.to_netcdf(folders+'/output/merged_day/NLDASconstant_' + v +'_latin.nc')\n",
    "        del param_ens\n",
    "        del all_ds \n",
    "        del all_merged\n",
    "        gc.collect()\n",
    "        print(v)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Here is the code to run a smaller test set that is not in distributed mode if you have errors and want to see them.\n",
    "s = ps.Simulation(executable, file_manager)\n",
    "s.manager['simStartTime'] = '1980-04-01 01:00'\n",
    "s.manager['simEndTime'] = '1980-04-03 00:00'\n",
    "for key in set(samples):\n",
    "    s.trial_params[key].values =  samples.get(key)\n",
    "print(s.decisions)\n",
    "s.run('local', run_suffix='_default')\n",
    "assert s.status == 'Success'"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(s.stderr)\n",
    "print(s.stdout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# Manipulating the Configuration of the pySUMMA Objects\n",
    "\n",
    "We need to run the parameter space with other model configurations, to see if the results seen on the default configuration hold true across the parameter space. The new configurations follow the exploration of [this paper.](https://doi.org/10.1002/2015WR017200)\n",
    "\n",
    "Clark, M.P., Nijssen, B., Lundquist, J.D., Kavetski, D., Rupp, D.E., Woods, R.A., Freer, J.E., Gutmann, E.D., Wood, A.W., Gochis, D.J. and Rasmussen, R.M., 2015. A unified approach for process‐based hydrologic modeling: 2. Model implementation and case studies. Water Resources Research, 51(4), pp.2515-2542.\n",
    "\n",
    "Of the model configurations discussed in this paper, the decisions that made the most difference are:\n",
    "\n",
    " - `groundwatr` choice of groundwater parameterization as:\n",
    "   - `qTopmodl` the topmodel parameterization (note must set hc_profile = pow_prof and bcLowrSoiH = zeroFlux\n",
    "   - `bigBuckt` a big bucket (lumped aquifer model) in between the other two choices for complexity\n",
    "   - `noXplict` no explicit groundwater parameterization\n",
    " - `stomResist` choice of function for stomatal resistance as:\n",
    "   - `BallBerry` Ball-Berry (1987) parameterization of physiological factors controlling transpiration\n",
    "   - `Jarvis` Jarvis (1976) parameterization of physiological factors controlling transpiration\n",
    "   - `simpleResistance` parameterized solely as a function of soil moisture limitations\n",
    " - `snowIncept` choice of parameterization for snow interception as:\n",
    "   - `stickySnow` maximum interception capacity is an increasing function of temerature\n",
    "   - `lightSnow` maximum interception capacity is an inverse function of new snow density\n",
    " - `windPrfile` choice of wind profile as:\n",
    "   - `exponential` an exponential wind profile extends to the surface\n",
    "   - `logBelowCanopy` a logarithmic profile below the vegetation canopy\n",
    "\n",
    "Choices `bigBuckt`, `BallBerry`, `lightSnow`, and `logBelowCanopy` are the defaults that we have run already (see the decisions printed out in the previous cell). The paper showed choice of `groundwatr` affecting the timing of runoff and the magnitude of evapotranspiration, `stomResist` affecting timing and magnitude of evapotranspiration, `snowIncept` affecting the magnitude canopy interception of snow, and `windPrfile` affecting the timing and magnitude of SWE, and latent and sensible heat. We will not explore the `groundwatr` configurations here as the differences show up only in most simulated variables post-calibration. This study does not examine models calibrated for every set up of forcings configurations. Note, if you want to look at `qTopmodl`, you must set  `bcLowrSoiH` to `zeroFlux` (we will leave it at `drainage`) and `hc_profile` to `pow_prof` (we will leave it at `constant`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if lhc_config_prob==1:\n",
    "    file_manager = folders+'/file_manager_truth.txt'\n",
    "    s = ps.Simulation(executable, file_manager)\n",
    "    #Before running the ensemble we must write the original simulation's configuration.\n",
    "    s._write_configuration()\n",
    "    print(s.decisions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if lhc_config_prob==1:\n",
    "    #alld = {'groundwatr':np.array(['qTopmodl','bigBuckt']),\n",
    "    alld = {'stomResist':np.array(['BallBerry','Jarvis']),\n",
    "            'snowIncept':np.array(['stickySnow','lightSnow']),\n",
    "            'windPrfile':np.array(['exponential','logBelowCanopy'])}\n",
    "    config = ps.ensemble.decision_product(alld)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "The ensemble uses `++` as a delimiter to create unique identifiers for each simulation in the ensemble. The default configuration will be run again. We do this so that each finished SUMMA *.nc output file is complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make ensembles with parameter space (numl parameter sets plus 1 for default)\n",
    "if lhc_config_prob==1:\n",
    "    config_latin = {}\n",
    "    for key_config in config.keys():\n",
    "        c = config[key_config]\n",
    "        for key_latin in latin.keys():\n",
    "            l = latin[key_latin]\n",
    "            config_latin[key_config+key_latin] = {**c,**l}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "Now, run these on the other forcing setups and parameter sets. Each iteration takes around 25 minutes with 4 basins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# truth, parameter space with 8 configurations\n",
    "if lhc_config_prob==1:\n",
    "    param_ens = ps.Ensemble(executable, config_latin, file_manager, num_workers=NCORES, client=client)   \n",
    "    param_ens.run('local')\n",
    "    all_status = [(n, s.status) for n, s in param_ens.simulations.items()] #if want to look at status if has errors\n",
    "    all_ds = [s.output.load() for n, s in param_ens.simulations.items()] #load it into memory so faster  \n",
    "    all_name = [n for n, s in param_ens.simulations.items()]\n",
    "    all_merged = xr.concat(all_ds, pd.Index(all_name, name=\"decision\"))\n",
    "    all_merged.to_netcdf(folders+'/output/merged_day/NLDAStruth_configs_latin.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that all were a success\n",
    "if lhc_config_prob==1:\n",
    "    print(all_status)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "To make sure things look how we want, we plot cumulative variables to see how differences are compounding.\n",
    "We plot one HRU (the first one) for 2 months. We are showing winter months, but this can be switched to summer months."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot cummulative\n",
    "if lhc_config_prob==1:\n",
    "    fig, axes = plt.subplots(nrows=14, ncols=1, figsize=(20, 40))\n",
    "    axes = axes.flatten()\n",
    "    axes[0].set_title('Cumulative')\n",
    "\n",
    "    variables = list(all_merged.variables.keys())[7:22]\n",
    "\n",
    "    #start =  24*5*30 #summer\n",
    "    start =  24*11*30 #winter\n",
    "    stop = start + 2*30*24 \n",
    "\n",
    "    truth_plt = all_merged.isel(hru=0, time=slice(start+90*24, stop+90*24)) #.cumsum(dim='time')\n",
    "    #truth_plt = all_merged.isel(hru=0).cumsum(dim='time')\n",
    "\n",
    "    for idx, var in enumerate(variables[0:14]):\n",
    "        for i, dec in enumerate(all_name):    \n",
    "            truth_plt[var].isel(decision=i).plot(ax=axes[idx],label=dec)\n",
    "        axes[idx].set_title('') \n",
    "        axes[idx].set_ylabel(var)\n",
    "        axes[idx].set_xlabel('Date')\n",
    "    plt.tight_layout()\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#delete things to save memory as before\n",
    "if lhc_config_prob==1:\n",
    "    del param_ens\n",
    "    del all_ds \n",
    "    del all_merged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "You might have to restart the following loop with several smaller subsets of `constant_vars`. \n",
    "It takes a long time to run so the Client times out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Each forcing constant, parameter space with 8 configurations.\n",
    "if lhc_config_prob==1:\n",
    "    constant_vars= ['airpres','airtemp','LWRadAtm','pptrate','spechum','SWRadAtm','windspd']\n",
    "    for v in constant_vars:\n",
    "        ! rm -rf /glade/work/ashleyvb/CAMELs/summa_camels/.pysumma \n",
    "        file_manager = folders+'/file_manager_constant_' + v +'.txt'\n",
    "        s = ps.Simulation(executable, file_manager)\n",
    "        s._write_configuration()\n",
    "        param_ens = ps.Ensemble(executable, config_latin, file_manager, num_workers=NCORES, client=client)   \n",
    "        param_ens.run('local')\n",
    "        all_status = [(n, s.status) for n, s in param_ens.simulations.items()] #if want to look at status if has errors\n",
    "        all_ds = [s.output.load() for n, s in param_ens.simulations.items()] #load it into memory so faster  \n",
    "        all_name = [n for n, s in param_ens.simulations.items()]\n",
    "        all_merged = xr.concat(all_ds, pd.Index(all_name, name=\"decision\"))\n",
    "        all_merged.to_netcdf(folders+'/output/merged_day/NLDASconstant_' + v +'_configs_latin.nc')\n",
    "        del param_ens\n",
    "        del all_ds \n",
    "        del all_merged\n",
    "        gc.collect()\n",
    "        print(v)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Here is the code to run a smaller test set that is not in distributed mode if you have errors and want to see them.\n",
    "s = ps.Simulation(executable, file_manager)\n",
    "s.manager['simStartTime'] = '1980-04-01 01:00'\n",
    "s.manager['simEndTime'] = '1980-04-03 00:00'\n",
    "print(s.decisions)\n",
    "s.run('local', run_suffix='_default')\n",
    "assert s.status == 'Success'"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(s.stderr)\n",
    "print(s.stdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pysumma",
   "language": "python",
   "name": "pysumma"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
