{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running the SUMMA setups\n",
    "To begin, we have to regionalize the paths in the configuration files that SUMMA will use.\n",
    "This is accomplished by running a shell command. This is done by starting a line with the `!` operator.\n",
    "We simply run a script to complete the installation.\n",
    "Then, we can import some basic libraries along with `pysumma`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### You will need to edit these paths to be your folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top = '/glade/work/ashleyvb'\n",
    "folder = top+'/CAMELs'\n",
    "folders = folder+'/summa_camels'\n",
    "! cd /glade/work/ashleyvb/CAMELs/summa_camels; ./install_local_setup.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we check that we loaded the correct environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conda list pysumma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "Then we load the imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pysumma as ps\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "from dask_jobqueue import PBSCluster\n",
    "from dask.distributed import Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NCORES=48\n",
    "cluster = PBSCluster(n_workers = NCORES,\n",
    "                     cores=NCORES,\n",
    "                     processes=NCORES, \n",
    "                     memory=\"24GB\",\n",
    "                     project='UWAS0091',\n",
    "                     queue='regular',\n",
    "                     walltime='06:00:00')\n",
    "client = Client(cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check that have workers, do not run the rest of the cells until the workers show up. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(client)\n",
    "!qstat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# Interacting with SUMMA via the `Distributed` object\n",
    "\n",
    "We are running a `Distributed` object, which has multiple `Simulation` objects inside, each corresponding to some spatial chunk. \n",
    "We need to do `rm -r /glade/work/ashleyvb/CAMELs/summa_camels/.pysumma/` to clear out the distributed folders every run so permissions do not get screwed up in the loops. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for fewer basins, do not exceed number of basins in chunking\n",
    "CHUNK = 8 #for all 671 basins\n",
    "# get number of HRUs\n",
    "attrib = xr.open_dataset(folders+'/settings.v1/attributes.nc')\n",
    "the_hru = np.array(attrib['hruId'])\n",
    "if len(the_hru) <8: CHUNK = len(the_hru)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "To set up a `Distributed` object you must supply several pieces of information. \n",
    "First, supply the SUMMA executable; this could be either the compiled executable on your local machine, or a docker image. \n",
    "The second piece of information is the path to the file manager, which we just created through the install script. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "executable = top+'/summa/bin/summa.exe'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_manager = folders+'/file_manager_truth.txt'\n",
    "camels = ps.Distributed(executable, file_manager, num_workers=NCORES, chunk_size=CHUNK, client=client)\n",
    "print(camels.manager) #possible days 1980-01-01 to 2018-12-31, we are running 1986-10-01 01:00 to 1991-10-02 0:00"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# pySUMMA with all Forcing Files\n",
    "\n",
    "We run pySumma for each set of forcing files on each basin. You can check how long it has been running by using the command `qstat -u <username>` in a terminal. Each run takes about 9 minutes for 671 basins (shorter if a subset). First, we start with the original NLDAS files, or the \"truth run\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "camels.run('local')\n",
    "#all_status = [(n, s.status) for n, s in camels.simulations.items()] #if want to look at status if has errors\n",
    "all_ds = [s.output.load() for n, s in camels.simulations.items()] #load it into memory so faster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "We could just write it as several files instead of merging. However, if we want to merge, we can do the following.\n",
    "First, detect automatically which vars have hru vs gru dimensions (depending on what we use for output, we may not have any gru):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hru_vars = [] # variables that have hru dimension\n",
    "gru_vars = [] # variables that have gru dimension\n",
    "for ds in all_ds:\n",
    "    for name, var in ds.variables.items():\n",
    "        if 'hru' in var.dims:\n",
    "            hru_vars.append(name)\n",
    "        elif 'gru' in var.dims:\n",
    "            gru_vars.append(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "Filter variables for merge, this takes seconds since we are running a limiited output, but if you add more to the output it will take longer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "hru_ds = [ds[hru_vars] for ds in all_ds]\n",
    "gru_ds = [ds[gru_vars] for ds in all_ds]\n",
    "hru_merged = xr.concat(hru_ds, dim='hru')\n",
    "gru_merged = xr.concat(gru_ds, dim='gru')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(hru_merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "hru_merged.to_netcdf(folders+'/output/merged_day/NLDAStruth_hru.nc')\n",
    "gru_merged.to_netcdf(folders+'/output/merged_day/NLDAStruth_gru.nc')\n",
    "del camels\n",
    "del all_ds \n",
    "del hru_merged\n",
    "del gru_merged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "Here are the other runs, now as a loop. The processes are the same, but for clarity we will divide it into 2 loops, one for the constant forcings and one for the MetSim forcings. This will take about an hour for each loop using all 671 basins. We delete stuff after every run to reduce memory needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Constant\n",
    "constant_vars= ['airpres','airtemp','LWRadAtm','pptrate','spechum','SWRadAtm','windspd','all']\n",
    "for v in constant_vars:\n",
    "    ! rm -rf /glade/work/ashleyvb/CAMELs/summa_camels/.pysumma \n",
    "    file_manager = folders+'/file_manager_constant_' + v +'.txt'\n",
    "    camels = ps.Distributed(executable, file_manager, num_workers=NCORES, chunk_size=CHUNK, client=client)   \n",
    "    camels.run('local')\n",
    "    #all_status = [(n, s.status) for n, s in camels.simulations.items()] #if want to look at status if has errors\n",
    "    all_ds = [s.output.load() for n, s in camels.simulations.items()] #load it into memory so faster    \n",
    "    hru_vars = [] # variables that have hru dimension\n",
    "    gru_vars = [] # variables that have gru dimension\n",
    "    for ds in all_ds:\n",
    "        for name, var in ds.variables.items():\n",
    "            if 'hru' in var.dims:\n",
    "                hru_vars.append(name)\n",
    "            elif 'gru' in var.dims:\n",
    "                gru_vars.append(name)\n",
    "    hru_ds = [ds[hru_vars] for ds in all_ds]\n",
    "    gru_ds = [ds[gru_vars] for ds in all_ds]\n",
    "    hru_merged = xr.concat(hru_ds, dim='hru')\n",
    "    gru_merged = xr.concat(gru_ds, dim='gru')\n",
    "    hru_merged.to_netcdf(folders+'/output/merged_day/NLDASconstant_' + v +'_hru.nc')\n",
    "    gru_merged.to_netcdf(folders+'/output/merged_day/NLDASconstant_' + v +'_gru.nc')\n",
    "    del camels\n",
    "    del all_ds \n",
    "    del hru_merged\n",
    "    del gru_merged\n",
    "    print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Metsim\n",
    "metsim_vars= ['airpres','airtemp','LWRadAtm','pptrate','spechum','SWRadAtm','windspd','all']\n",
    "for v in metsim_vars:\n",
    "    ! rm -rf /glade/work/ashleyvb/CAMELs/summa_camels/.pysumma \n",
    "    file_manager = folders+'/file_manager_metsim_' + v +'.txt'\n",
    "    camels = ps.Distributed(executable, file_manager, num_workers=NCORES, chunk_size=CHUNK, client=client)   \n",
    "    camels.run('local')\n",
    "    #all_status = [(n, s.status) for n, s in camels.simulations.items()] #if want to look at status if has errors\n",
    "    all_ds = [s.output.load() for n, s in camels.simulations.items()] #load it into memory so faster    \n",
    "    hru_vars = [] # variables that have hru dimension\n",
    "    gru_vars = [] # variables that have gru dimension\n",
    "    for ds in all_ds:\n",
    "        for name, var in ds.variables.items():\n",
    "            if 'hru' in var.dims:\n",
    "                hru_vars.append(name)\n",
    "            elif 'gru' in var.dims:\n",
    "                gru_vars.append(name)\n",
    "    hru_ds = [ds[hru_vars] for ds in all_ds]\n",
    "    gru_ds = [ds[gru_vars] for ds in all_ds]\n",
    "    hru_merged = xr.concat(hru_ds, dim='hru')\n",
    "    gru_merged = xr.concat(gru_ds, dim='gru')\n",
    "    hru_merged.to_netcdf(folders+'/output/merged_day/NLDASmetsim_' + v +'_hru.nc')\n",
    "    gru_merged.to_netcdf(folders+'/output/merged_day/NLDASmetsim_' + v +'_gru.nc')\n",
    "    del camels\n",
    "    del all_ds \n",
    "    del hru_merged\n",
    "    del gru_merged\n",
    "    print(v)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Here is the code to run a smaller test set that is not in distributed mode if you have errors and want to see them.\n",
    "s = ps.Simulation(executable, file_manager)\n",
    "#s.decisions['simulStart'] = '1980-04-02 00:00'\n",
    "#s.decisions['simulFinsh'] = '1980-04-02 23:00'\n",
    "print(s.decisions)\n",
    "s.run('local', run_suffix='_default')\n",
    "assert s.status == 'Success'"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(s.stderr)\n",
    "print(s.stdout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# Manipulating the Configuration of the pySUMMA Objects\n",
    "\n",
    "Currrently, none of the model decisions (or parameters) can be altered in a `Distributed` object. \n",
    "However, if we switch to `Simulation` objects and use the `Ensemble` class, we can run suites of different model configurations with relative ease. \n",
    "This code would take a long time to run on all 671 basins, so we throw an error if you try to run it with more than 10 basins (but theoretically you could run it with as many as you want). \n",
    "Above is a good stopping point for this notebook if you plan on running the entire dataset, and then you can move on to running the notebook `camels_analyze_entire_output.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(the_hru) >10: raise SystemExit(\"Stop right there!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "    \n",
    "Since we must have a small subset of basins, we will proceed with the ensemble calculations.\n",
    "Afterwards, you can run the notebook `camels_analyze_subset_output.ipynb`.\n",
    "Before running the ensemble though, change to a simulation re-write the original simulation's configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_manager = folders+'/file_manager_truth.txt'\n",
    "s = ps.Simulation(executable, file_manager)\n",
    "s._write_configuration()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "The configurations follow the exploration of [this paper.](https://doi.org/10.1002/2015WR017200)\n",
    "\n",
    "Clark, M.P., Nijssen, B., Lundquist, J.D., Kavetski, D., Rupp, D.E., Woods, R.A., Freer, J.E., Gutmann, E.D., Wood, A.W., Gochis, D.J. and Rasmussen, R.M., 2015. A unified approach for process‐based hydrologic modeling: 2. Model implementation and case studies. Water Resources Research, 51(4), pp.2515-2542.\n",
    "\n",
    "Of the model configurations discussed in this paper, the decisions that made the most difference are:\n",
    "\n",
    " - `groundwatr` choice of groundwater parameterization as:\n",
    "   - `qTopmodl` the topmodel parameterization (note must set hc_profile = pow_prof and bcLowrSoiH = zeroFlux\n",
    "   - `bigBuckt` a big bucket (lumped aquifer model) in between the other two choices for complexity\n",
    "   - `noXplict` no explicit groundwater parameterization\n",
    " - `snowIncept` choice of parameterization for snow interception as:\n",
    "   - `stickySnow` maximum interception capacity is an increasing function of temerature\n",
    "   - `lightSnow` maximum interception capacity is an inverse function of new snow density\n",
    " - `windPrfile` choice of wind profile as\n",
    "   - `exponential` an exponential wind profile extends to the surface\n",
    "   - `logBelowCanopy` a logarithmic profile below the vegetation canopy\n",
    "\n",
    "Choices `bigBuckt`, `lightSnow`, and `logBelowCanopy` are the defaults that we have run already. The paper showed choice of `groundwatr` affecting the timing of runoff and the magnitude of evapotranspiration, `snowIncept` affecting the magnitude canopy interception of snow, and `windPrfile` affecting the timing and magnitude of SWE, and latent and sensible heat. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# qTopmodl vs bigBuckt groundwater only make difference in baseflow var, snowIncept and windPrfile makes a difference a few places\n",
    "\n",
    "#alld = {'stomResist':np.array(['BallBerry','Jarvis']),'snowLayers':np.array(['jrdn1991','CLM_2010'])}\n",
    "# Andrew recommended, layers doesn't seem to do anything with defaults\n",
    "\n",
    "alld = {'groundwatr':np.array(['qTopmodl','bigBuckt']),'stomResist':np.array(['BallBerry','Jarvis']),'snowIncept':np.array(['stickySnow','lightSnow']),'windPrfile':np.array(['exponential','logBelowCanopy'])}\n",
    "# according to paper\n",
    "\n",
    "config = ps.ensemble.decision_product(alld)\n",
    "param_ens = ps.Ensemble(executable, config, file_manager, num_workers=2, client=client) #8 eventually for 8 sims"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Now we just do what we did before in the simulations previously, except here we merge with a new dimension of the the configuration decision identifier instead of by `hru` and `gru`. The ensemble uses `++` as a delimiter to create unique identifiers for each simulation in the enemble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "param_ens.run('local')\n",
    "all_status = [(n, s.status) for n, s in param_ens.simulations.items()] #if want to look at status if has errors\n",
    "all_ds = [s.output.load() for n, s in param_ens.simulations.items()] #load it into memory so faster  \n",
    "all_name = [n for n, s in param_ens.simulations.items()]\n",
    "all_merged = xr.concat(all_ds, pd.Index(all_name, name=\"decision\"))\n",
    "#all_merged.to_netcdf(folders+'/output/merged_day/NLDAStruth_configs.nc')\n",
    "#del param_ens\n",
    "#del all_ds \n",
    "#del all_merged\n",
    "print(all_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plot cummulative\n",
    "fig, axes = plt.subplots(nrows=14, ncols=1, figsize=(20, 40))\n",
    "axes = axes.flatten()\n",
    "axes[0].set_title('Cumulative')\n",
    "\n",
    "variables = list(all_merged.variables.keys())[7:22]\n",
    "\n",
    "#start =  24*5*30 #summer\n",
    "start =  24*10*30 #winter\n",
    "stop = start + 1*100*24 \n",
    "\n",
    "truth_plt = all_merged.isel(hru=0, time=slice(start+90*24, stop+90*24)) #.cumsum(dim='time')\n",
    "#truth_plt = all_merged.isel(hru=0).cumsum(dim='time')\n",
    "\n",
    "for idx, var in enumerate(variables[0:14]):\n",
    "    for i, dec in enumerate(all_name):    \n",
    "        truth_plt[var].isel(decision=i).plot(ax=axes[idx],label=dec)\n",
    "    axes[idx].set_title('') \n",
    "    axes[idx].set_ylabel(var)\n",
    "    axes[idx].set_xlabel('D2ate')\n",
    "plt.tight_layout()\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_manager = folders+'/file_manager_truth.txt'\n",
    "s = ps.Simulation(executable, file_manager)\n",
    "s.manager['simStartTime'] = '1980-04-01 01:00'\n",
    "s.manager['simEndTime'] = '1980-04-03 00:00'\n",
    "#s.decisions['groundwatr'] = 'qTopmodl'\n",
    "#s.decisions['hc_profile'] = 'pow_prof'\n",
    "#s.decisions['bcLowrSoiH'] = 'zeroFlux'\n",
    "#s.decisions['vegeParTbl'] = 'USGS'\n",
    "#s.decisions['veg_traits'] = 'CM_QJRMS1988'\n",
    "#s.decisions['LAI_method'] = 'specified'\n",
    "print(s.decisions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s.run('local', run_suffix='_default')\n",
    "assert s.status == 'Success'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(s.stderr)\n",
    "print(s.stdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pysumma",
   "language": "python",
   "name": "pysumma"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
