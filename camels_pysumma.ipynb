{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running the SUMMA setup\n",
    "To begin, we have to regionalize the paths in the configuration files that SUMMA will use.\n",
    "This is accomplished by running a shell command. This is done by starting a line with the `!` operator.\n",
    "We simply run a script to complete the installation.\n",
    "Then, we can import some basic libraries along with `pysumma`.\n",
    "The `%pylab inline` magic command simply imports some standard scientific packages such as `numpy` and `matplotlib`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### You will need to edit these paths to be your folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top = '/glade/work/ashleyvb'\n",
    "folder = top+'/CAMELs'\n",
    "folders = folder+'/summa_camels'\n",
    "! cd /glade/work/ashleyvb/CAMELs/summa_camels; ./install_local_setup.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we check that we loaded the correct environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conda list pysumma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "Then we load the imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "import pysumma as ps\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "from dask_jobqueue import PBSCluster\n",
    "from dask.distributed import Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NCORES=48\n",
    "cluster = PBSCluster(n_workers = NCORES,\n",
    "                     cores=NCORES,\n",
    "                     processes=NCORES, \n",
    "                     memory=\"24GB\",\n",
    "                     project='UWAS0091',\n",
    "                     queue='regular',\n",
    "                     walltime='06:00:00')\n",
    "client = Client(cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check that have workers, do not run the rest of the cells until the workers show up. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(client)\n",
    "!qstat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <br>\n",
    "\n",
    "# Interacting with SUMMA via the `Distributed` object\n",
    "\n",
    "We are running a `Distributed` object, which has multiple `Simulation` objects inside, each corresponding to some spatial chunk. \n",
    "We need to do `rm -r /glade/work/ashleyvb/CAMELs/summa_camels/.pysumma/` to clear out the distributed folders every run so permissions do not get screwed up in the loops. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for fewer basins, do not exceed number of basins in chunking\n",
    "CHUNK = 8 #for all 671 basins\n",
    "# get number of HRUs\n",
    "attrib = xr.open_dataset(folders+'/settings.v1/attributes.nc')\n",
    "the_hru = np.array(attrib['hruId'])\n",
    "if len(the_hru) <8: CHUNK = len(the_hru)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Instantiating a `Distributed` object\n",
    "\n",
    "To set up a `Distributed` object you must supply several pieces of information. \n",
    "First, supply the SUMMA executable; this could be either the compiled executable on your local machine, or a docker image. \n",
    "The second piece of information is the path to the file manager, which we just created through the install script. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "executable = top+'/summa/bin/summa.exe'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_manager = folders+'/file_manager_truth.txt'\n",
    "camels = ps.Distributed(executable, file_manager, num_workers=NCORES, chunk_size=CHUNK, client=client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Manipulating the configuration of the distributed object\n",
    "\n",
    "Currrently, none of the attributes that can be changed in a SUMMA `Simulation` object cannot be altered in a `Distributed` object and the only one that can be viewed is the file manager. In the notebook that made the forcing files, we wrote file managers. \n",
    "To see a file manager, simply `print` it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(camels.manager) #possible days 1980-01-01 to 2018-12-31, we are running 1986-10-01 01:00 to 1991-10-02 0:00"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Running pySUMMA for all Forcing Files\n",
    "\n",
    "We now run all 671 basins of pySUMMA for each set of forcing files. You can check how long it has been running by using the command `qstat -u <username>` in a terminal. Each run takes about 9 minutes. First, we start with the original NLDAS files, or the \"truth run\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "camels.run('local')\n",
    "#all_status = [(n, s.status) for n, s in camels.simulations.items()] #if want to look at status if has errors\n",
    "all_ds = [s.output.load() for n, s in camels.simulations.items()] #load it into memory so faster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "We could just write it as several files instead of merging. However, if we want to merge, we can do the following.\n",
    "First, detect automatically which vars have hru vs gru dimensions (depending on what we use for output, we may not have any gru):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hru_vars = [] # variables that have hru dimension\n",
    "gru_vars = [] # variables that have gru dimension\n",
    "for ds in all_ds:\n",
    "    for name, var in ds.variables.items():\n",
    "        if 'hru' in var.dims:\n",
    "            hru_vars.append(name)\n",
    "        elif 'gru' in var.dims:\n",
    "            gru_vars.append(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "Filter variables for merge, this takes seconds since we are running a limiited output, but if you add more to the output it will take longer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "hru_ds = [ds[hru_vars] for ds in all_ds]\n",
    "gru_ds = [ds[gru_vars] for ds in all_ds]\n",
    "hru_merged = xr.concat(hru_ds, dim='hru')\n",
    "gru_merged = xr.concat(gru_ds, dim='gru')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(hru_merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "hru_merged.to_netcdf(folders+'/output/merged_day/NLDAStruth_hru.nc')\n",
    "gru_merged.to_netcdf(folders+'/output/merged_day/NLDAStruth_gru.nc')\n",
    "del camels\n",
    "del all_ds \n",
    "del hru_merged\n",
    "del gru_merged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "Here are the other runs, now as a loop. The processes are the same, but for clarity we will divide it into 2 loops, one for the constant forcings and one for the MetSim forcings. This will take about an hour for each loop using all 671 basins. We delete stuff after every run to reduce memory needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Constant\n",
    "constant_vars= ['airpres','airtemp','LWRadAtm','pptrate','spechum','SWRadAtm','windspd','all']\n",
    "for v in constant_vars:\n",
    "    ! rm -rf /glade/work/ashleyvb/CAMELs/summa_camels/.pysumma \n",
    "    file_manager = folders+'/file_manager_constant_' + v +'.txt'\n",
    "    camels = ps.Distributed(executable, file_manager, num_workers=NCORES, chunk_size=CHUNK, client=client)   \n",
    "    camels.run('local')\n",
    "    #all_status = [(n, s.status) for n, s in camels.simulations.items()] #if want to look at status if has errors\n",
    "    all_ds = [s.output.load() for n, s in camels.simulations.items()] #load it into memory so faster    \n",
    "    hru_vars = [] # variables that have hru dimension\n",
    "    gru_vars = [] # variables that have gru dimension\n",
    "    for ds in all_ds:\n",
    "        for name, var in ds.variables.items():\n",
    "            if 'hru' in var.dims:\n",
    "                hru_vars.append(name)\n",
    "            elif 'gru' in var.dims:\n",
    "                gru_vars.append(name)\n",
    "    hru_ds = [ds[hru_vars] for ds in all_ds]\n",
    "    gru_ds = [ds[gru_vars] for ds in all_ds]\n",
    "    hru_merged = xr.concat(hru_ds, dim='hru')\n",
    "    gru_merged = xr.concat(gru_ds, dim='gru')\n",
    "    hru_merged.to_netcdf(folders+'/output/merged_day/NLDASconstant_' + v +'_hru.nc')\n",
    "    gru_merged.to_netcdf(folders+'/output/merged_day/NLDASconstant_' + v +'_gru.nc')\n",
    "    del camels\n",
    "    del all_ds \n",
    "    del hru_merged\n",
    "    del gru_merged\n",
    "    print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Metsim\n",
    "metsim_vars= ['airpres','airtemp','LWRadAtm','pptrate','spechum','SWRadAtm','windspd','all']\n",
    "for v in metsim_vars:\n",
    "    ! rm -rf /glade/work/ashleyvb/CAMELs/summa_camels/.pysumma \n",
    "    file_manager = folders+'/file_manager_metsim_' + v +'.txt'\n",
    "    camels = ps.Distributed(executable, file_manager, num_workers=NCORES, chunk_size=CHUNK, client=client)   \n",
    "    camels.run('local')\n",
    "    #all_status = [(n, s.status) for n, s in camels.simulations.items()] #if want to look at status if has errors\n",
    "    all_ds = [s.output.load() for n, s in camels.simulations.items()] #load it into memory so faster    \n",
    "    hru_vars = [] # variables that have hru dimension\n",
    "    gru_vars = [] # variables that have gru dimension\n",
    "    for ds in all_ds:\n",
    "        for name, var in ds.variables.items():\n",
    "            if 'hru' in var.dims:\n",
    "                hru_vars.append(name)\n",
    "            elif 'gru' in var.dims:\n",
    "                gru_vars.append(name)\n",
    "    hru_ds = [ds[hru_vars] for ds in all_ds]\n",
    "    gru_ds = [ds[gru_vars] for ds in all_ds]\n",
    "    hru_merged = xr.concat(hru_ds, dim='hru')\n",
    "    gru_merged = xr.concat(gru_ds, dim='gru')\n",
    "    hru_merged.to_netcdf(folders+'/output/merged_day/NLDASmetsim_' + v +'_hru.nc')\n",
    "    gru_merged.to_netcdf(folders+'/output/merged_day/NLDASmetsim_' + v +'_gru.nc')\n",
    "    del camels\n",
    "    del all_ds \n",
    "    del hru_merged\n",
    "    del gru_merged\n",
    "    print(v)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Here is the code to run a smaller test set that is not in distributed mode if you have errors and want to see them.\n",
    "s = ps.Simulation(executable, file_manager)\n",
    "#s.decisions['simulStart'] = '1980-04-02 00:00'\n",
    "#s.decisions['simulFinsh'] = '1980-04-02 23:00'\n",
    "print(s.decisions)\n",
    "s.run('local', run_suffix='_default')\n",
    "assert s.status == 'Success'"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(s.stderr)\n",
    "print(s.stdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pysumma",
   "language": "python",
   "name": "pysumma"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
