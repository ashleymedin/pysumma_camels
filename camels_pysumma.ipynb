{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running the SUMMA setups\n",
    "To begin, we have to regionalize the paths in the configuration files that SUMMA will use.\n",
    "This is accomplished by running a shell command. This is done by starting a line with the `!` operator.\n",
    "We simply run a script to complete the installation.\n",
    "Then, we can import some basic libraries along with `pysumma`.\n",
    "Note, if you run this with all 671 basins, you may be maxing out memory. \n",
    "If things start to fail, restart the server and run the simuluations in smaller loops before restarts. Also, make sure you start with 16GB per node on the Cheyenne PBS Job Options. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "Check that we loaded the correct environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# packages in environment at /glade/work/ashleyvb/miniconda3/envs/pysumma:\n",
      "#\n",
      "# Name                    Version                   Build  Channel\n",
      "pysumma                   3.0.0                     dev_0    <develop>\n",
      "summa                     3.0.0               h0f1b500_10    conda-forge\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "conda list summa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "Load the imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pysumma as ps\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "from dask_jobqueue import PBSCluster\n",
    "from dask.distributed import Client\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### You will need to edit these paths to be your folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some folder places\n",
    "home = '/glade/work/ashleyvb/CAMELs'\n",
    "sm_home = '/glade/work/ashleyvb/summa'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "Keep these the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_folder = home+'/summa_camels'\n",
    "settings_folder = top_folder+'/settings.v1'\n",
    "ps_working = top_folder+'/.pysumma'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "! cd {top_folder}; chmod +x install_local_setup.sh; ./install_local_setup.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Make problem complexity choices here:\n",
    "\n",
    "Now decide if you want to run: \n",
    " - `default_prob = 1`: the \"default\" configuration with the \"default\" parameters. By \"default\" we mean whatever you chose in the summa setup files. \n",
    " - `lhc_prob = 1`: the default configuration with exploration of the parameter space.\n",
    " - `lhc_config_prob = 1`: 8 different configurations (choices that have been seen to affect the model output in previous research) with exploration of the parameter space.\n",
    " \n",
    "Note, you can choose all of these to be 1, but each step of complexity will contain the previous problem(s), so it is not necessary. It will result in more files, but if may be useful to check that each step of the problem expansion runs successfullly. If you have more than 10 HRUs (CAMELs subbasins in the example problem), it was decided that the problem is too big and you can only run the default problem. However, theoretically it will run with more HRUs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get number of HRUs\n",
    "attrib = xr.open_dataset(settings_folder+'/attributes.nc')\n",
    "the_hru = np.array(attrib['hruId'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_prob = 0\n",
    "lhc_prob = 0\n",
    "lhc_config_prob = 1\n",
    "if len(the_hru) >10:\n",
    "    lhc_prob = 0\n",
    "    lhc_config_prob = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# smaller problem needs less power, if your problem is particularly small you might be able to get by with even less and not make the kernel die.\n",
    "# You can also add more time, up to 06:00:00\n",
    "if len(the_hru) >10:\n",
    "    NCORES=48\n",
    "    cluster = PBSCluster(n_workers = NCORES,\n",
    "                     cores=NCORES,\n",
    "                     processes=NCORES, \n",
    "                     memory=\"24GB\",\n",
    "                     project='UWAS0091',\n",
    "                     queue='share',\n",
    "                     walltime='03:00:00')\n",
    "else:\n",
    "    NCORES=32\n",
    "    cluster = PBSCluster(n_workers = NCORES,\n",
    "                         cores=NCORES,\n",
    "                         processes=NCORES, \n",
    "                         memory=\"16GB\",\n",
    "                         project='UWAS0091',\n",
    "                         queue='share',\n",
    "                         walltime='03:00:00')\n",
    "\n",
    "client = Client(cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check that have workers, do not run the rest of the cells until the workers show up. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Client: 'tcp://10.148.10.15:45001' processes=0 threads=0, memory=0 B>\n",
      "Job id            Name             User              Time Use S Queue\n",
      "----------------  ---------------- ----------------  -------- - -----\n",
      "5047474.chadmin1  Jupyter          ashleyvb          00:00:09 R shareex         \n",
      "5047478.chadmin1  dask-worker      ashleyvb                 0 Q shareex         \n"
     ]
    }
   ],
   "source": [
    "print(client)\n",
    "!qstat\n",
    "# put Job id in here and run if workers are not showing up\n",
    "# !qdel $Job id$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# Interacting with SUMMA via the `Distributed` object\n",
    "\n",
    "We are running a `Distributed` object, which has multiple `Simulation` objects inside, each corresponding to some spatial chunk. \n",
    "We need to do `rm -r /glade/work/ashleyvb/CAMELs/summa_camels/.pysumma/` to clear out the distributed folders every run so permissions do not get screwed up in the loops. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for fewer basins, do not exceed number of basins in chunking\n",
    "CHUNK = 8 #for all 671 basins\n",
    "if len(the_hru) <8: CHUNK = len(the_hru)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "To set up a `Distributed` object you must supply several pieces of information. \n",
    "First, supply the SUMMA executable; this could be either the compiled executable on your local machine, or a docker image. \n",
    "The second piece of information is the path to the file manager, which we just created through the install script. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "executable = sm_home+'/bin/summa.exe'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "controlVersion                       'SUMMA_FILE_MANAGER_V3.0.0'\n",
      "simStartTime                         '1990-10-01 00:00'\n",
      "simEndTime                           '1996-09-30 23:00'\n",
      "tmZoneInfo                           'utcTime'\n",
      "settingsPath                         '/glade/work/ashleyvb/CAMELs/summa_camels/settings.v1/'\n",
      "forcingPath                          '/glade/work/ashleyvb/CAMELs/summa_camels/forcing/truth/'\n",
      "outputPath                           '/glade/work/ashleyvb/CAMELs/summa_camels/output/truth/'\n",
      "decisionsFile                        'modelDecisions.1hr.txt'\n",
      "outputControlFile                    'output_control2.txt'\n",
      "globalHruParamFile                   '../settings.v1/localParamInfo.txt'\n",
      "globalGruParamFile                   '../settings.v1/basinParamInfo.txt'\n",
      "attributeFile                        '../settings.v1/attributes.nc'\n",
      "trialParamFile                       '../settings.v1/parameters.nc'\n",
      "forcingListFile                      '../settings.v1/forcingFileList.truth.txt'\n",
      "initConditionFile                    '../settings.v1/init_cond.nc'\n",
      "outFilePrefix                        'camels_truth'\n",
      "vegTableFile                         'VEGPARM.TBL'\n",
      "soilTableFile                        'SOILPARM.TBL'\n",
      "generalTableFile                     'GENPARM.TBL'\n",
      "noahmpTableFile                      'MPTABLE.TBL'\n"
     ]
    }
   ],
   "source": [
    "file_manager = top_folder+'/file_manager_truth.txt'\n",
    "camels = ps.Distributed(executable, file_manager, num_workers=NCORES, chunk_size=CHUNK, client=client)\n",
    "print(camels.manager) #possible days 1980-01-01 to 2018-12-31, we are running 1986-10-01 01:00 to 1991-10-02 0:00"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# pySUMMA with all Forcing Files\n",
    "\n",
    "We run pySumma for each set of forcing files on each basin. You can check how long it has been running by using the command `qstat -u <username>` in a terminal. Each run takes about 9 minutes for 671 basins (shorter if a subset). First, we start with the original NLDAS files, or the \"truth run\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 4 µs, total: 4 µs\n",
      "Wall time: 6.44 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# truth, default parameters and configuration\n",
    "if default_prob==1:\n",
    "    camels.run('local')\n",
    "    all_status = [(n, s.status) for n, s in camels.simulations.items()] #if want to look at status if has errors\n",
    "    all_ds = [s.output.load() for n, s in camels.simulations.items()] #load it into memory so faster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "We could just write it as several files instead of merging. However, if we want to merge, we can do the following.\n",
    "First, detect automatically which vars have hru vs gru dimensions (depending on what we use for output, we may not have any gru):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "if default_prob==1:\n",
    "    hru_vars = [] # variables that have hru dimension\n",
    "    gru_vars = [] # variables that have gru dimension\n",
    "    for ds in all_ds:\n",
    "        for name, var in ds.variables.items():\n",
    "            if 'hru' in var.dims:\n",
    "                hru_vars.append(name)\n",
    "            elif 'gru' in var.dims:\n",
    "                gru_vars.append(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "Filter variables for merge, this takes seconds since we are running a limiited output, but if you add more to the output it will take longer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 µs, sys: 0 ns, total: 4 µs\n",
      "Wall time: 5.25 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if default_prob==1:\n",
    "    hru_ds = [ds[hru_vars] for ds in all_ds]\n",
    "    gru_ds = [ds[gru_vars] for ds in all_ds]\n",
    "    hru_merged = xr.concat(hru_ds, dim='hru')\n",
    "    gru_merged = xr.concat(gru_ds, dim='gru')\n",
    "    print(hru_merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 µs, sys: 1 µs, total: 3 µs\n",
      "Wall time: 5.96 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if default_prob==1:\n",
    "    hru_merged.to_netcdf(top_folder+'/output/merged_day/NLDAStruth_hru.nc')\n",
    "    gru_merged.to_netcdf(top_folder+'/output/merged_day/NLDAStruth_gru.nc')\n",
    "    del camels\n",
    "    del all_ds \n",
    "    del hru_merged\n",
    "    del gru_merged\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "Here are the other runs with each forcing held constant, now as a loop. This will take about an hour for each loop using all 671 basins, and about a minute for each loop using 4 basins. We delete stuff after every run to reduce memory needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 µs, sys: 0 ns, total: 4 µs\n",
      "Wall time: 6.2 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Each forcing constant, default parameters and configuration\n",
    "if default_prob==1:\n",
    "    constant_vars= ['airpres','airtemp','LWRadAtm','pptrate','spechum','SWRadAtm','windspd']\n",
    "    for v in constant_vars:\n",
    "        ! rm -rf {ps_working}\n",
    "        file_manager = top_folder+'/file_manager_constant_' + v +'.txt'\n",
    "        camels = ps.Distributed(executable, file_manager, num_workers=NCORES, chunk_size=CHUNK, client=client)   \n",
    "        camels.run('local')\n",
    "        all_status = [(n, s.status) for n, s in camels.simulations.items()] #if want to look at status if has errors\n",
    "        all_ds = [s.output.load() for n, s in camels.simulations.items()] #load it into memory so faster    \n",
    "        hru_vars = [] # variables that have hru dimension\n",
    "        gru_vars = [] # variables that have gru dimension\n",
    "        for ds in all_ds:\n",
    "            for name, var in ds.variables.items():\n",
    "                if 'hru' in var.dims:\n",
    "                    hru_vars.append(name)\n",
    "                elif 'gru' in var.dims:\n",
    "                    gru_vars.append(name)\n",
    "        hru_ds = [ds[hru_vars] for ds in all_ds]\n",
    "        gru_ds = [ds[gru_vars] for ds in all_ds]\n",
    "        hru_merged = xr.concat(hru_ds, dim='hru')\n",
    "        gru_merged = xr.concat(gru_ds, dim='gru')\n",
    "        hru_merged.to_netcdf(top_folder+'/output/merged_day/NLDASconstant_' + v +'_hru.nc')\n",
    "        gru_merged.to_netcdf(top_folder+'/output/merged_day/NLDASconstant_' + v +'_gru.nc')\n",
    "        del camels\n",
    "        del all_ds \n",
    "        del hru_merged\n",
    "        del gru_merged\n",
    "        gc.collect()\n",
    "        print(v)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Here is the code to run a smaller test set that is not in distributed mode if you have errors and want to see them.\n",
    "s = ps.Simulation(executable, file_manager)\n",
    "s.manager['simStartTime'] = '1980-04-01 01:00'\n",
    "s.manager['simEndTime'] = '1980-04-03 00:00'\n",
    "print(s.decisions)\n",
    "s.run('local', run_suffix='_default')\n",
    "assert s.status == 'Success'"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(s.stderr)\n",
    "print(s.stdout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# Exploring the Parameter Calibration Space with a Latin Hyper cube\n",
    "\n",
    "The above models were run on the default parameter set only. Let's rerun with parameter sets selected by using a Latin Hyper cube to get 10 different parameter sets for every HRU, in order to explore the calibration space. This will show us if the results of forcing importance could change after calibration. \n",
    "\n",
    "Currrently, none of the model parameters (or decisions) can be altered in a `Distributed` object. \n",
    "However, if we switch to `Simulation` objects and use the `Ensemble` class, we can run suites of different model parameters with relative ease. \n",
    "\n",
    "We change only the parameters that are usually calibrated. You can remove parameters if you were not planning to ever calibrate them away from their defaults (likewise you could add parameters).\n",
    "\n",
    "The absolute minimums and maximums will break simulations and zero out variables, so we do not use those, we stay at the 5% level away from the extremes. Also, there are some constraints on the parameters that must be followed, they are:\n",
    "\n",
    "* heightCanopyTop   > heightCanopyBottom\n",
    "* critSoilTranspire > theta_res\n",
    "* theta_sat         > critSoilTranspire\n",
    "* fieldCapacity     > theta_res\n",
    "* theta_sat         > fieldCapacity\n",
    "* theta_sat         > theta_res\n",
    "* critSoilTranspire > critSoilWilting\n",
    "* critSoilWilting   > theta_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "    \n",
    "Since we must have a small subset of basins, we will proceed with the ensemble calculations with the parameter space\n",
    "Afterwards, you can run the notebook `camels_analyze_subset_output.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "if lhc_prob==1 or lhc_config_prob==1: from pyDOE import lhs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "if lhc_prob==1 or lhc_config_prob==1:\n",
    "    file_manager = top_folder+'/file_manager_truth.txt'\n",
    "    s = ps.Simulation(executable, file_manager)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "albedoRefresh             |       1.0000 |       1.0000 |      10.0000\n",
      "aquiferBaseflowExp        |       2.0000 |       1.0000 |      10.0000\n",
      "aquiferBaseflowRate       |       0.1000 |       0.0000 |       0.1000\n",
      "frozenPrecipMultip        |       1.0000 |       0.5000 |       1.5000\n",
      "heightCanopyBottom        |       2.0000 |       0.0000 |       5.0000\n",
      "heightCanopyTop           |      20.0000 |       0.0500 |     100.0000\n",
      "k_macropore               |       0.0010 |       1.0d-7 |       0.1000\n",
      "k_soil                    |       7.5d-6 |       1.0d-7 |       1.0d-5\n",
      "qSurfScale                |      50.0000 |       1.0000 |     100.0000\n",
      "summerLAI                 |       3.0000 |       0.0100 |      10.0000\n",
      "tempCritRain              |     273.1600 |     272.1600 |     274.1600\n",
      "theta_sat                 |       0.5500 |       0.3000 |       0.6000\n",
      "windReductionParam        |       0.2800 |       0.0000 |       1.0000\n",
      "routingGammaScale         |       2.0d+4 |       1.0000 |       1.0d+5\n",
      "routingGammaShape         |       2.5000 |       2.0000 |       3.0000\n"
     ]
    }
   ],
   "source": [
    "# print the default, min, and max as in /settings.v1/localParamInfo.txt and /settings.v1/basinParamInfo.txt\n",
    "if lhc_prob==1 or lhc_config_prob==1:\n",
    "    param_calib_hru = ['albedoRefresh', 'aquiferBaseflowExp', 'aquiferBaseflowRate', 'frozenPrecipMultip', 'heightCanopyBottom','heightCanopyTop', 'k_macropore', \n",
    "                   'k_soil', 'qSurfScale', 'summerLAI', 'tempCritRain', 'theta_sat', 'windReductionParam'] \n",
    "    param_calib_gru = ['routingGammaScale', 'routingGammaShape']\n",
    "\n",
    "    for k in param_calib_hru:\n",
    "        print(s.global_hru_params[k])\n",
    "    for k in param_calib_gru:\n",
    "        print(s.global_gru_params[k]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "if lhc_prob==1 or lhc_config_prob==1:\n",
    "    bounds_hru = np.full((len(param_calib_hru),3),1.0)\n",
    "    bounds_gru = np.full((len(param_calib_gru),3),1.0)\n",
    "    for i,k in enumerate(param_calib_hru): bounds_hru[i,]= s.global_hru_params.get_value(k)[0:3]\n",
    "    for i,k in enumerate(param_calib_gru): bounds_gru[i,]= s.global_gru_params.get_value(k)[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define bounds and expand to size of LHS runs\n",
    "if lhc_prob==1 or lhc_config_prob==1:\n",
    "    numl = 10\n",
    "    num_vars =  len(param_calib_hru) + len(param_calib_gru)\n",
    "    names =  param_calib_hru + param_calib_gru\n",
    "    bounds =  np.concatenate((bounds_hru, bounds_gru), axis=0)\n",
    "    par_def = dict(zip(names, np.transpose(np.tile(bounds[:,0],(len(the_hru),1))) ))\n",
    "    par_min = dict(zip(names, np.transpose(np.tile(bounds[:,1],(numl*len(the_hru),1))) ))\n",
    "    par_max = dict(zip(names, np.transpose(np.tile(bounds[:,2],(numl*len(the_hru),1))) ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure to obey parameter constraints\n",
    "if lhc_prob==1 or lhc_config_prob==1:\n",
    "    param = xr.open_dataset(settings_folder+'/parameters.nc')\n",
    "\n",
    "    for i,h in enumerate(the_hru):\n",
    "        lb_theta_sat = max(param[['critSoilTranspire','fieldCapacity','theta_res']].isel(hru=i).values()).values\n",
    "        for j in range(0,numl): #say first numl belong to hru 0, second numl to hru 1, and so on\n",
    "            if (par_min['theta_sat'][j + i*numl]<lb_theta_sat): par_min['theta_sat'][j + i*numl]=lb_theta_sat\n",
    "\n",
    "    par_min['heightCanopyTop'] = par_max['heightCanopyBottom']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a 5% buffer\n",
    "if lhc_prob==1 or lhc_config_prob==1:\n",
    "    buff = {key: (par_max.get(key) - par_min.get(key))*0.05 for key in set(par_max) }\n",
    "    par_min = {key: par_min.get(key) + buff.get(key)*0.05 for key in set(buff) }\n",
    "    par_max = {key: par_max.get(key) - buff.get(key)*0.05 for key in set(buff) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate samples with Latin Hypercube Sampling\n",
    "if lhc_prob==1 or lhc_config_prob==1:\n",
    "    np.random.seed(1111)\n",
    "    lhd = lhs(numl*len(the_hru), samples=num_vars)\n",
    "    lhd = dict(zip(names,lhd))\n",
    "    samples = {key: par_min.get(key) + lhd.get(key)*(par_max.get(key) - par_min.get(key)) for key in set(par_max) }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "Set 0 will be the default parameter set. This means the the default parameter set will be run again. We do this so that each finished SUMMA *.nc output file is complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make ensembles\n",
    "if lhc_prob==1 or lhc_config_prob==1:\n",
    "    latin = {}\n",
    "    latin[str(0)] = {'trial_parameters': {key: par_def.get(key) for key in set(par_def) }}\n",
    "    for j in range(0,numl):\n",
    "            latin[str(j+1)] = {'trial_parameters': {key: samples.get(key)[np.arange(j, len(the_hru)*numl, numl)] for key in set(samples) }}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Now we just do what we did before in the simulations previously, except here we merge with a new dimension of the the configuration decision identifier instead of by `hru` and `gru`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 µs, sys: 0 ns, total: 4 µs\n",
      "Wall time: 5.96 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# truth, parameter space with default configuration\n",
    "if lhc_prob==1:\n",
    "    param_ens = ps.Ensemble(executable, latin, file_manager, num_workers=NCORES, client=client)   \n",
    "    param_ens.run('local')\n",
    "    all_status = [(n, s.status) for n, s in param_ens.simulations.items()] #if want to look at status if has errors\n",
    "    all_ds = [s.output.load() for n, s in param_ens.simulations.items()] #load it into memory so faster  \n",
    "    all_name = [n for n, s in param_ens.simulations.items()]\n",
    "    all_merged = xr.concat(all_ds, pd.Index(all_name, name=\"decision\"))\n",
    "    all_merged.to_netcdf(top_folder+'/output/merged_day/NLDAStruth_latin.nc')\n",
    "    del param_ens\n",
    "    del all_ds \n",
    "    del all_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that all were a success\n",
    "if lhc_prob==1:\n",
    "    print(all_status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 µs, sys: 1 µs, total: 3 µs\n",
      "Wall time: 7.15 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Each forcing constant, parameter space with default configuration\n",
    "if lhc_prob==1:\n",
    "    constant_vars= ['airpres','airtemp','LWRadAtm','pptrate','spechum','SWRadAtm','windspd']\n",
    "    for v in constant_vars:\n",
    "        ! rm -rf {ps_working}\n",
    "        file_manager = top_folder+'/file_manager_constant_' + v +'.txt'\n",
    "        s = ps.Simulation(executable, file_manager)\n",
    "        s._write_configuration()\n",
    "        param_ens = ps.Ensemble(executable, latin, file_manager, num_workers=NCORES, client=client)   \n",
    "        param_ens.run('local')\n",
    "        all_status = [(n, s.status) for n, s in param_ens.simulations.items()] #if want to look at status if has errors\n",
    "        all_ds = [s.output.load() for n, s in param_ens.simulations.items()] #load it into memory so faster  \n",
    "        all_name = [n for n, s in param_ens.simulations.items()]\n",
    "        all_merged = xr.concat(all_ds, pd.Index(all_name, name=\"decision\"))\n",
    "        all_merged.to_netcdf(top_folder+'/output/merged_day/NLDASconstant_' + v +'_latin.nc')\n",
    "        del param_ens\n",
    "        del all_ds \n",
    "        del all_merged\n",
    "        gc.collect()\n",
    "        print(v)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Here is the code to run a smaller test set that is not in distributed mode if you have errors and want to see them.\n",
    "s = ps.Simulation(executable, file_manager)\n",
    "s.manager['simStartTime'] = '1980-04-01 01:00'\n",
    "s.manager['simEndTime'] = '1980-04-03 00:00'\n",
    "for key in set(samples):\n",
    "    s.trial_params[key].values =  samples.get(key)\n",
    "print(s.decisions)\n",
    "s.run('local', run_suffix='_default')\n",
    "assert s.status == 'Success'"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(s.stderr)\n",
    "print(s.stdout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# Manipulating the Configuration of the pySUMMA Objects\n",
    "\n",
    "We need to run the parameter space with other model configurations, to see if the results seen on the default configuration hold true across the parameter space. The new configurations follow the exploration of [this paper.](https://doi.org/10.1002/2015WR017200)\n",
    "\n",
    "Clark, M.P., Nijssen, B., Lundquist, J.D., Kavetski, D., Rupp, D.E., Woods, R.A., Freer, J.E., Gutmann, E.D., Wood, A.W., Gochis, D.J. and Rasmussen, R.M., 2015. A unified approach for process‐based hydrologic modeling: 2. Model implementation and case studies. Water Resources Research, 51(4), pp.2515-2542.\n",
    "\n",
    "Of the model configurations discussed in this paper, the decisions that made the most difference are:\n",
    "\n",
    " - `groundwatr` choice of groundwater parameterization as:\n",
    "   - `qTopmodl` the topmodel parameterization (note must set hc_profile = pow_prof and bcLowrSoiH = zeroFlux\n",
    "   - `bigBuckt` a big bucket (lumped aquifer model) in between the other two choices for complexity\n",
    "   - `noXplict` no explicit groundwater parameterization\n",
    " - `stomResist` choice of function for stomatal resistance as:\n",
    "   - `BallBerry` Ball-Berry (1987) parameterization of physiological factors controlling transpiration\n",
    "   - `Jarvis` Jarvis (1976) parameterization of physiological factors controlling transpiration\n",
    "   - `simpleResistance` parameterized solely as a function of soil moisture limitations\n",
    " - `snowIncept` choice of parameterization for snow interception as:\n",
    "   - `stickySnow` maximum interception capacity is an increasing function of temerature\n",
    "   - `lightSnow` maximum interception capacity is an inverse function of new snow density\n",
    " - `windPrfile` choice of wind profile as:\n",
    "   - `exponential` an exponential wind profile extends to the surface\n",
    "   - `logBelowCanopy` a logarithmic profile below the vegetation canopy\n",
    "\n",
    "Choices `bigBuckt`, `BallBerry`, `lightSnow`, and `logBelowCanopy` are the defaults that we have run already (see the decisions printed out in the previous cell). The paper showed choice of `groundwatr` affecting the timing of runoff and the magnitude of evapotranspiration, `stomResist` affecting timing and magnitude of evapotranspiration, `snowIncept` affecting the magnitude canopy interception of snow, and `windPrfile` affecting the timing and magnitude of SWE, and latent and sensible heat. We will not explore the `groundwatr` configurations here as the differences show up only in most simulated variables post-calibration. This study does not examine models calibrated for every set up of forcings configurations. Note, if you want to look at `qTopmodl`, you must set  `bcLowrSoiH` to `zeroFlux` (we will leave it at `drainage`) and `hc_profile` to `pow_prof` (we will leave it at `constant`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "soilCatTbl    STAS                 ! soil-category dataset\n",
      "vegeParTbl    MODIFIED_IGBP_MODIS_NOAH ! vegetation-category dataset\n",
      "soilStress    NoahType             ! choice of function for the soil moisture control on stomatal resistance\n",
      "stomResist    BallBerry            ! choice of function for stomatal resistance\n",
      "num_method    itertive             ! choice of numerical method\n",
      "fDerivMeth    analytic             ! choice of method to calculate flux derivatives\n",
      "LAI_method    specified            ! choice of method to determine LAI and SAI\n",
      "f_Richards    mixdform             ! form of Richards equation\n",
      "groundwatr    bigBuckt             ! choice of groundwater parameterization\n",
      "hc_profile    constant             ! choice of hydraulic conductivity profile\n",
      "bcUpprTdyn    nrg_flux             ! type of upper boundary condition for thermodynamics\n",
      "bcLowrTdyn    zeroFlux             ! type of lower boundary condition for thermodynamics\n",
      "bcUpprSoiH    liq_flux             ! type of upper boundary condition for soil hydrology\n",
      "bcLowrSoiH    drainage             ! type of lower boundary condition for soil hydrology\n",
      "veg_traits    Raupach_BLM1994      ! choice of parameterization for vegetation roughness length and displacement height\n",
      "canopyEmis    difTrans             ! choice of parameterization for canopy emissivity\n",
      "snowIncept    lightSnow            ! choice of parameterization for snow interception\n",
      "windPrfile    logBelowCanopy       ! choice of canopy wind profile\n",
      "astability    louisinv             ! choice of stability function\n",
      "canopySrad    BeersLaw             ! choice of method for canopy shortwave radiation\n",
      "alb_method    conDecay             ! choice of albedo representation\n",
      "compaction    anderson             ! choice of compaction routine\n",
      "snowLayers    CLM_2010             ! choice of method to combine and sub-divide snow layers\n",
      "thCondSnow    jrdn1991             ! choice of thermal conductivity representation for snow\n",
      "thCondSoil    funcSoilWet          ! choice of thermal conductivity representation for soil\n",
      "spatial_gw    localColumn          ! choice of method for spatial representation of groundwater\n",
      "subRouting    timeDlay             ! choice of method for sub-grid routing\n"
     ]
    }
   ],
   "source": [
    "if lhc_config_prob==1:\n",
    "    file_manager = top_folder+'/file_manager_truth.txt'\n",
    "    s = ps.Simulation(executable, file_manager)\n",
    "    #Before running the ensemble we must write the original simulation's configuration.\n",
    "    s._write_configuration()\n",
    "    print(s.decisions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "if lhc_config_prob==1:\n",
    "    #alld = {'groundwatr':np.array(['qTopmodl','bigBuckt']),\n",
    "    alld = {'stomResist':np.array(['BallBerry','Jarvis']),\n",
    "            'snowIncept':np.array(['stickySnow','lightSnow']),\n",
    "            'windPrfile':np.array(['exponential','logBelowCanopy'])}\n",
    "    config = ps.ensemble.decision_product(alld)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "The ensemble uses `++` as a delimiter to create unique identifiers for each simulation in the ensemble. The default configuration will be run again. We do this so that each finished SUMMA *.nc output file is complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make ensembles with parameter space (numl parameter sets plus 1 for default)\n",
    "if lhc_config_prob==1:\n",
    "    config_latin = {}\n",
    "    for key_config in config.keys():\n",
    "        c = config[key_config]\n",
    "        for key_latin in latin.keys():\n",
    "            l = latin[key_latin]\n",
    "            config_latin[key_config+key_latin] = {**c,**l}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "Now, run these on the other forcing setups and parameter sets. Each iteration takes around 25 minutes with 4 basins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# truth, parameter space with 8 configurations\n",
    "if lhc_config_prob==1:\n",
    "    param_ens = ps.Ensemble(executable, config_latin, file_manager, num_workers=NCORES, client=client)   \n",
    "    param_ens.run('local')\n",
    "    all_status = [(n, s.status) for n, s in param_ens.simulations.items()] #if want to look at status if has errors\n",
    "    all_ds = [s.output.load() for n, s in param_ens.simulations.items()] #load it into memory so faster  \n",
    "    all_name = [n for n, s in param_ens.simulations.items()]\n",
    "    all_merged = xr.concat(all_ds, pd.Index(all_name, name=\"decision\"))\n",
    "    all_merged.to_netcdf(top_folder+'/output/merged_day/NLDAStruth_configs_latin.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that all were a success\n",
    "if lhc_config_prob==1:\n",
    "    print(all_status)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "To make sure things look how we want, we plot cumulative variables to see how differences are compounding.\n",
    "We plot one HRU (the first one) for 2 months. We are showing winter months, but this can be switched to summer months."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot cummulative\n",
    "if lhc_config_prob==1:\n",
    "    fig, axes = plt.subplots(nrows=14, ncols=1, figsize=(20, 40))\n",
    "    axes = axes.flatten()\n",
    "    axes[0].set_title('Cumulative')\n",
    "\n",
    "    variables = list(all_merged.variables.keys())[7:22]\n",
    "\n",
    "    #start =  24*5*30 #summer\n",
    "    start =  24*11*30 #winter\n",
    "    stop = start + 2*30*24 \n",
    "\n",
    "    truth_plt = all_merged.isel(hru=0, time=slice(start+90*24, stop+90*24)) #.cumsum(dim='time')\n",
    "    #truth_plt = all_merged.isel(hru=0).cumsum(dim='time')\n",
    "\n",
    "    for idx, var in enumerate(variables[0:14]):\n",
    "        for i, dec in enumerate(all_name):    \n",
    "            truth_plt[var].isel(decision=i).plot(ax=axes[idx],label=dec)\n",
    "        axes[idx].set_title('') \n",
    "        axes[idx].set_ylabel(var)\n",
    "        axes[idx].set_xlabel('Date')\n",
    "    plt.tight_layout()\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#delete things to save memory as before\n",
    "if lhc_config_prob==1:\n",
    "    del param_ens\n",
    "    del all_ds \n",
    "    del all_merged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "You might have to restart the following loop with several smaller subsets of `constant_vars`. \n",
    "It takes a long time to run so the Client times out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Each forcing constant, parameter space with 8 configurations.\n",
    "if lhc_config_prob==1:\n",
    "    constant_vars= ['airpres','airtemp','LWRadAtm','pptrate','spechum','SWRadAtm','windspd']\n",
    "    for v in constant_vars:\n",
    "        ! rm -rf {ps_working}\n",
    "        file_manager = top_folder+'/file_manager_constant_' + v +'.txt'\n",
    "        s = ps.Simulation(executable, file_manager)\n",
    "        s._write_configuration()\n",
    "        param_ens = ps.Ensemble(executable, config_latin, file_manager, num_workers=NCORES, client=client)   \n",
    "        param_ens.run('local')\n",
    "        all_status = [(n, s.status) for n, s in param_ens.simulations.items()] #if want to look at status if has errors\n",
    "        all_ds = [s.output.load() for n, s in param_ens.simulations.items()] #load it into memory so faster  \n",
    "        all_name = [n for n, s in param_ens.simulations.items()]\n",
    "        all_merged = xr.concat(all_ds, pd.Index(all_name, name=\"decision\"))\n",
    "        all_merged.to_netcdf(top_folder+'/output/merged_day/NLDASconstant_' + v +'_configs_latin.nc')\n",
    "        del param_ens\n",
    "        del all_ds \n",
    "        del all_merged\n",
    "        gc.collect()\n",
    "        print(v)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Here is the code to run a smaller test set that is not in distributed mode if you have errors and want to see them.\n",
    "s = ps.Simulation(executable, file_manager)\n",
    "s.manager['simStartTime'] = '1980-04-01 01:00'\n",
    "s.manager['simEndTime'] = '1980-04-03 00:00'\n",
    "print(s.decisions)\n",
    "s.run('local', run_suffix='_default')\n",
    "assert s.status == 'Success'"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(s.stderr)\n",
    "print(s.stdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:miniconda3-pysumma]",
   "language": "python",
   "name": "conda-env-miniconda3-pysumma-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
